{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "273b9dba-de39-49f9-9c5b-2dedc33cf6d3",
      "metadata": {
        "tags": [],
        "id": "273b9dba-de39-49f9-9c5b-2dedc33cf6d3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import emdatabase as emdb\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.io import mmread\n",
        "from scipy.spatial import cKDTree\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, f1_score, adjusted_rand_score, ConfusionMatrixDisplay\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import maxfuse.match_utils as match_utils\n",
        "from maxfuse import utils\n",
        "from maxfuse.model import Fusor\n",
        "\n",
        "import anndata as ad\n",
        "import scanpy as sc\n",
        "import maxfuse as mf\n",
        "import tangram as tg\n",
        "import pickle\n",
        "\n",
        "import seaborn as sns\n",
        "import json\n",
        "import plotly.express as px\n",
        "import os\n",
        "import requests\n",
        "import gseapy as gp\n",
        "import blitzgsea as blitz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c688d8-f609-4c69-8d73-a2855bc1d5fa",
      "metadata": {
        "id": "41c688d8-f609-4c69-8d73-a2855bc1d5fa"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "The file format for MaxFuse to read in is adata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14dc7e3-6230-4e13-95b0-e56d04e85bb9",
      "metadata": {
        "id": "b14dc7e3-6230-4e13-95b0-e56d04e85bb9"
      },
      "outputs": [],
      "source": [
        "target_acq = 'FinalLiv-27_c002_v001_r001_reg012' # main sample tissue\n",
        "\n",
        "seg_df = emdb.get_cell_segmentation_output_for_acquisition_id(target_acq, 1)\n",
        "bio_df = emdb.get_cell_biomarker_expression_for_acquisition_id(target_acq, 1)\n",
        "class_df = pd.read_csv(f'scratch/SP_SC_data/s4769_cell_types/{target_acq}.cell_types.csv')\n",
        "\n",
        "seg_bio_df = pd.merge(seg_df, bio_df, on='CELL_ID')\n",
        "protein = pd.merge(seg_bio_df, class_df, on='CELL_ID')\n",
        "\n",
        "biomarker_columns = ['BCL6', 'CCR7', 'CD107a', 'CD11c', 'CD138',\n",
        "                     'CD14', 'CD141', 'CD15', 'CD163', 'CD1c',\n",
        "                     'CD20', 'CD204', 'CD206', 'CD209', 'CD21',\n",
        "                     'CD3', 'CD31', 'CD34', 'CD4', 'CD45',\n",
        "                     'CD45RO', 'CD56', 'CD68', 'CD79a', 'CD8',\n",
        "                     'CXCL13', 'CXCR5', 'DAPI', 'DCLAMP', 'FAP',\n",
        "                     'FOXP3', 'GRB', 'HLADR', 'ICOS', 'IDO1',\n",
        "                     'IFNg', 'INOS', 'KERATIN 8_18', 'Ki67',\n",
        "                     'LAG3', 'PD1', 'PDL1', 'PNAD', 'Podoplanin',\n",
        "                     'TCF1', 'TOX', 'Tbet', 'VISTA', 'XCR1',\n",
        "                     'aSMA', 'eomes']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "protein[biomarker_columns] = scaler.fit_transform(protein[biomarker_columns])\n",
        "\n",
        "print(protein[biomarker_columns].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57aa920-9742-4294-8195-f781e1d79e26",
      "metadata": {
        "tags": [],
        "id": "a57aa920-9742-4294-8195-f781e1d79e26"
      },
      "outputs": [],
      "source": [
        "# check how CODEX data looks like\n",
        "sns.scatterplot(data=protein, x=\"X\", y=\"Y\", hue = \"CELL_TYPE\", s = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aeb7682-a469-4102-a18f-db39955e6f71",
      "metadata": {
        "tags": [],
        "id": "1aeb7682-a469-4102-a18f-db39955e6f71"
      },
      "outputs": [],
      "source": [
        "columns = bio_df.columns\n",
        "protein_features = [col for col in columns if col != 'CELL_ID']\n",
        "\n",
        "# convert to AnnData\n",
        "protein_adata = ad.AnnData(\n",
        "    protein[protein_features].to_numpy(), dtype=np.float32\n",
        ")\n",
        "protein_adata.var_names = protein[protein_features].columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d622e16-2531-49d9-8f68-9cd71d0cf224",
      "metadata": {
        "tags": [],
        "id": "2d622e16-2531-49d9-8f68-9cd71d0cf224"
      },
      "outputs": [],
      "source": [
        "# read in RNA data\n",
        "rna = mmread(\"scratch/SP_SC_data/liver_rna_counts.txt\") # rna count as sparse matrix\n",
        "rna_names = pd.read_csv('scratch/SP_SC_data/liver_rna_names.csv')['names'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f100bfd-df17-45ee-b4d2-e18f9284f91e",
      "metadata": {
        "id": "5f100bfd-df17-45ee-b4d2-e18f9284f91e"
      },
      "outputs": [],
      "source": [
        "# convert to AnnData\n",
        "rna_adata = ad.AnnData(\n",
        "    rna.tocsr(), dtype=np.float32\n",
        ")\n",
        "rna_adata.var_names = rna_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ffb0f9-934c-4972-8597-8e51565844d9",
      "metadata": {
        "id": "87ffb0f9-934c-4972-8597-8e51565844d9"
      },
      "outputs": [],
      "source": [
        "# process all RNA features\n",
        "sc.pp.normalize_total(rna_adata)\n",
        "sc.pp.log1p(rna_adata)\n",
        "sc.pp.highly_variable_genes(rna_adata, n_top_genes=5000)\n",
        "# only retain highly variable genes\n",
        "rna_adata = rna_adata[:, rna_adata.var.highly_variable].copy()\n",
        "sc.pp.scale(rna_adata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a21fcd7-7cdb-4db4-aee4-7305580e80ca",
      "metadata": {
        "id": "7a21fcd7-7cdb-4db4-aee4-7305580e80ca"
      },
      "outputs": [],
      "source": [
        "# scRNA clustering for further refining\n",
        "# PCA\n",
        "sc.tl.pca(rna_adata, svd_solver='arpack')\n",
        "\n",
        "# compute neighborhood graph\n",
        "sc.pp.neighbors(rna_adata, n_neighbors=10, n_pcs=40)\n",
        "\n",
        "# run Leiden clustering\n",
        "sc.tl.leiden(rna_adata, resolution=0.5)\n",
        "\n",
        "# visualize the clustering\n",
        "sc.tl.umap(rna_adata)\n",
        "sc.pl.umap(rna_adata, color=['leiden'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aedfcf8-fcf7-48dc-a743-2eedb48017c3",
      "metadata": {
        "tags": [],
        "id": "0aedfcf8-fcf7-48dc-a743-2eedb48017c3"
      },
      "outputs": [],
      "source": [
        "# read in celltyle labels\n",
        "\n",
        "metadata_rna = pd.read_csv('scratch/SP_SC_data/liver_rna_meta.csv')\n",
        "labels_rna = metadata_rna['Type'].to_numpy()\n",
        "labels_codex = protein['CELL_TYPE'].to_numpy()\n",
        "\n",
        "protein_adata.obs['celltype'] = labels_codex\n",
        "rna_adata.obs['celltype'] = labels_rna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1d0594-4528-4b54-9854-e620ca2a7eca",
      "metadata": {
        "id": "ad1d0594-4528-4b54-9854-e620ca2a7eca"
      },
      "outputs": [],
      "source": [
        "# define marker genes for hierarchical cell type refinement\n",
        "marker_genes = {\n",
        "    'CD4 T cells': ['CD4', 'FOXP3', 'CD40LG', 'CCR7', 'IL2RA'],\n",
        "    'CD8 T cells': ['CD8A', 'CD8B', 'GZMB', 'PRF1', 'EOMES',\n",
        "                    'KLRG1'],\n",
        "    'Macrophages M2-like': ['CD163', 'MRC1', 'CD206'],\n",
        "    'Macrophages': ['CD68', 'ITGAM', 'CD14']\n",
        "}\n",
        "\n",
        "def annotate_clusters(sub_adata, marker_genes):\n",
        "    # annotate clusters based on marker genes\n",
        "    cluster_annotations = {}\n",
        "    for cluster in sub_adata.obs['leiden'].unique():\n",
        "        cluster_cells = sub_adata[sub_adata.obs['leiden'] == cluster]\n",
        "        scores = {}\n",
        "        for cell_type, markers in marker_genes.items():\n",
        "            scores[cell_type] = np.mean([np.mean(cluster_cells[:, marker].X) for marker in markers if marker in cluster_cells.var_names])\n",
        "        # annotate cluster with the cell type with the highest score\n",
        "        best_match = max(scores, key=scores.get)\n",
        "        cluster_annotations[cluster] = best_match\n",
        "\n",
        "    # map cluster annotations to the data\n",
        "    refined_cell_types = [cluster_annotations[cl] for cl in sub_adata.obs['leiden']]\n",
        "    return refined_cell_types\n",
        "\n",
        "# ensure new categories are included in the 'celltype' column\n",
        "new_categories = ['CD4 T cells', 'CD8 T cells', 'Macrophages M2-like', 'Macrophages']\n",
        "rna_adata.obs['celltype'] = rna_adata.obs['celltype'].astype('category')\n",
        "rna_adata.obs['celltype'] = rna_adata.obs['celltype'].cat.add_categories(new_categories)\n",
        "\n",
        "# separate and annotate T cells and TAMs\n",
        "t_cells = rna_adata[rna_adata.obs['celltype'] == 'T cells']\n",
        "t_cell_annotations = annotate_clusters(t_cells, {'CD4 T cells': marker_genes['CD4 T cells'], 'CD8 T cells': marker_genes['CD8 T cells']})\n",
        "rna_adata.obs.loc[t_cells.obs.index, 'celltype'] = t_cell_annotations\n",
        "\n",
        "tams = rna_adata[rna_adata.obs['celltype'] == 'TAMs']\n",
        "tams_annotations = annotate_clusters(tams, {'Macrophages M2-like': marker_genes['Macrophages M2-like'], 'Macrophages': marker_genes['Macrophages']})\n",
        "rna_adata.obs.loc[tams.obs.index, 'celltype'] = tams_annotations\n",
        "\n",
        "# remove original 'T cells' and 'TAMs' categories and rename 'CAFs' to 'Fibroblasts'\n",
        "rna_adata.obs['celltype'] = rna_adata.obs['celltype'].cat.remove_categories(['T cells', 'TAMs'])\n",
        "rna_adata.obs['celltype'] = rna_adata.obs['celltype'].replace({'CAFs': 'Fibroblasts'})\n",
        "\n",
        "sc.pl.umap(rna_adata, color=['celltype'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6838ac7f-6324-4cc3-b308-cf8634ca500f",
      "metadata": {
        "id": "6838ac7f-6324-4cc3-b308-cf8634ca500f"
      },
      "outputs": [],
      "source": [
        "labels_rna = rna_adata.obs['celltype'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d03b0bc-a711-44a7-bd73-02afc80d7f1a",
      "metadata": {
        "id": "9d03b0bc-a711-44a7-bd73-02afc80d7f1a"
      },
      "outputs": [],
      "source": [
        "def build_sp_graph(protein_adata):\n",
        "    data_matrix = protein_adata.X\n",
        "    print(\"Shape of data_matrix:\", data_matrix.shape)\n",
        "\n",
        "    if len(data_matrix.shape) != 2:\n",
        "        raise ValueError(\"The extracted data matrix should be a 2D array.\")\n",
        "\n",
        "    G = nx.Graph()\n",
        "    for i in range(data_matrix.shape[0]):\n",
        "        G.add_node(i)\n",
        "\n",
        "    # calculate distances and add edges with weights\n",
        "    nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(data_matrix)\n",
        "    distances, indices = nn.kneighbors(data_matrix)\n",
        "\n",
        "    for i, neighbors in enumerate(indices):\n",
        "        for j, neighbor in enumerate(neighbors):\n",
        "            if i != neighbor:\n",
        "                G.add_edge(i, neighbor, weight=distances[i][j])\n",
        "\n",
        "    return G\n",
        "\n",
        "def precompute_nearest_neighbors(G):\n",
        "    # precompute 10 nearest neighbors for each node\n",
        "    nearest_neighbors = {}\n",
        "    for node in G.nodes:\n",
        "        neighbors = sorted(G[node], key=lambda x: G[node][x]['weight'])[:10]\n",
        "        nearest_neighbors[node] = neighbors\n",
        "    return nearest_neighbors\n",
        "\n",
        "def save_nearest_neighbors(nearest_neighbors, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(nearest_neighbors, file)\n",
        "\n",
        "G = build_sp_graph(protein_adata)\n",
        "nearest_neighbors = precompute_nearest_neighbors(G)\n",
        "save_nearest_neighbors(nearest_neighbors, 'scratch/nearest_neighbors.pkl')\n",
        "\n",
        "print(\"Nearest neighbors have been precomputed and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ce5d0d-ca3f-45cf-972b-6b8b1a8f3c2a",
      "metadata": {
        "id": "f7ce5d0d-ca3f-45cf-972b-6b8b1a8f3c2a"
      },
      "outputs": [],
      "source": [
        "# proteins to map\n",
        "protein_names = [\n",
        "    'BCL6', 'CXCL13', 'CXCR5', 'DAPI', 'DCLAMP', 'FAP', 'GRB',\n",
        "    'ICOS', 'IDO1', 'IFNg', 'INOS', 'KERATIN 8_18', 'LAG3',\n",
        "    'PNAD', 'TCF1', 'TOX', 'VISTA', 'eomes'\n",
        "]\n",
        "\n",
        "# function to query MyGene.info for protein-to-gene mappings\n",
        "def get_protein_to_gene_mapping(protein_names):\n",
        "    url = \"http://mygene.info/v3/query\"\n",
        "    mapping = {}\n",
        "\n",
        "    for protein in protein_names:\n",
        "        params = {\n",
        "            'q': protein,\n",
        "            'fields': 'symbol,name,uniprot',\n",
        "            'species': 'human',\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'hits' in data and len(data['hits']) > 0:\n",
        "            for item in data['hits']:\n",
        "                uniprot = item.get('uniprot', {})\n",
        "                if 'Swiss-Prot' in uniprot:\n",
        "                    mapping[protein] = item['symbol']\n",
        "                    break\n",
        "        else:\n",
        "            mapping[protein] = None\n",
        "\n",
        "    return mapping\n",
        "\n",
        "# get protein-to-gene mapping\n",
        "protein_to_gene_mapping = get_protein_to_gene_mapping(protein_names)\n",
        "\n",
        "mapping_df = pd.DataFrame(list(protein_to_gene_mapping.items()), columns=['Protein name', 'Gene name'])\n",
        "print(mapping_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81d6e8a-020d-4422-abec-7dd6619cb343",
      "metadata": {
        "tags": [],
        "id": "e81d6e8a-020d-4422-abec-7dd6619cb343"
      },
      "outputs": [],
      "source": [
        "# construct the feature correspondence\n",
        "correspondence = pd.read_csv('data/complete_protein_gene_conversion.csv')\n",
        "correspondence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04292b41-df24-4f2a-ad62-f40518c2f037",
      "metadata": {
        "id": "04292b41-df24-4f2a-ad62-f40518c2f037"
      },
      "outputs": [],
      "source": [
        "rna_protein_correspondence = []\n",
        "\n",
        "for i in range(correspondence.shape[0]):\n",
        "    curr_protein_name, curr_rna_names = correspondence.iloc[i]\n",
        "    if curr_protein_name not in protein_adata.var_names:\n",
        "        continue\n",
        "\n",
        "    if isinstance(curr_rna_names, float): # check if curr_rna_names is a float (e.g. NaN)\n",
        "        continue\n",
        "\n",
        "    if curr_rna_names.find('Ignore') != -1: # some correspondence ignored (e.g. protein isoform to one gene)\n",
        "        continue\n",
        "    curr_rna_names = curr_rna_names.split('/') # e.g. one protein to multiple genes\n",
        "    for r in curr_rna_names:\n",
        "        if r in rna_adata.var_names:\n",
        "            rna_protein_correspondence.append([r, curr_protein_name])\n",
        "\n",
        "rna_protein_correspondence = np.array(rna_protein_correspondence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95aba24d-869a-4ef0-9ce0-9da359601c14",
      "metadata": {
        "id": "95aba24d-869a-4ef0-9ce0-9da359601c14"
      },
      "outputs": [],
      "source": [
        "# Columns rna_shared and protein_shared are matched.\n",
        "# One may encounter \"Variable names are not unique\" warning,\n",
        "# this is fine and is because one RNA may encode multiple proteins and vice versa.\n",
        "# ensure unique column names\n",
        "rna_adata.var_names_make_unique()\n",
        "protein_adata.var_names_make_unique()\n",
        "# verify that all RNA names exist in rna_adata and all protein names exist in protein_adata\n",
        "rna_valid_names = [name for name in rna_protein_correspondence[:, 0] if name in rna_adata.var_names]\n",
        "protein_valid_names = [name for name in rna_protein_correspondence[:, 1] if name in protein_adata.var_names]\n",
        "\n",
        "# ensure there is a valid correspondence\n",
        "rna_protein_valid_correspondence = [(rna, protein) for rna, protein in zip(rna_protein_correspondence[:, 0], rna_protein_correspondence[:, 1]) if rna in rna_adata.var_names and protein in protein_adata.var_names]\n",
        "\n",
        "# unzip the valid correspondences\n",
        "rna_valid_names, protein_valid_names = zip(*rna_protein_valid_correspondence)\n",
        "\n",
        "# perform the indexing\n",
        "rna_shared = rna_adata[:, rna_valid_names].copy()\n",
        "protein_shared = protein_adata[:, protein_valid_names].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddefeb72-3683-4982-96ae-9239dbad1a62",
      "metadata": {
        "scrolled": true,
        "id": "ddefeb72-3683-4982-96ae-9239dbad1a62"
      },
      "outputs": [],
      "source": [
        "# Make sure no column is static\n",
        "mask = (\n",
        "    (rna_shared.X\n",
        "     # .toarray()\n",
        "     .std(axis=0) > 0.5)\n",
        "    & (protein_shared.X.std(axis=0) > 0.05)\n",
        ")\n",
        "rna_shared = rna_shared[:, mask].copy()\n",
        "protein_shared = protein_shared[:, mask].copy()\n",
        "print([rna_shared.shape,protein_shared.shape])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cf00d5-d8c6-4cef-aaed-5bf14725012d",
      "metadata": {
        "id": "47cf00d5-d8c6-4cef-aaed-5bf14725012d"
      },
      "outputs": [],
      "source": [
        "# plot UMAP of rna cells based only on rna markers with protein correspondence\n",
        "sc.pp.neighbors(rna_shared, n_neighbors=15)\n",
        "sc.tl.umap(rna_shared)\n",
        "sc.pl.umap(rna_shared, color='celltype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429c81ad-57c1-4c60-8dec-e1263c2af20d",
      "metadata": {
        "id": "429c81ad-57c1-4c60-8dec-e1263c2af20d"
      },
      "outputs": [],
      "source": [
        "# plot UMAPs of codex cells based only on protein markers with rna correspondence\n",
        "\n",
        "sc.pp.neighbors(protein_shared, n_neighbors=15)\n",
        "sc.tl.umap(protein_shared)\n",
        "sc.pl.umap(protein_shared, color='celltype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a503beb5-a23a-4210-8b0c-477ab822ffaa",
      "metadata": {
        "id": "a503beb5-a23a-4210-8b0c-477ab822ffaa"
      },
      "outputs": [],
      "source": [
        "rna_shared = rna_shared.X.copy()\n",
        "protein_shared = protein_shared.X.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819b3144-e424-44b5-98cc-83ebd81e0834",
      "metadata": {
        "id": "819b3144-e424-44b5-98cc-83ebd81e0834"
      },
      "outputs": [],
      "source": [
        "# plot UMAPs of rna cells based on all active rna markers\n",
        "rna_adata.raw = rna_adata\n",
        "sc.pp.neighbors(rna_adata, n_neighbors=15)\n",
        "sc.tl.umap(rna_adata)\n",
        "sc.pl.umap(rna_adata, color='celltype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332f7d4e-1714-46f9-8fed-cca8b9649ae7",
      "metadata": {
        "id": "332f7d4e-1714-46f9-8fed-cca8b9649ae7"
      },
      "outputs": [],
      "source": [
        "# plot UMAPs of protein cells based on all active protein markers\n",
        "\n",
        "sc.pp.neighbors(protein_adata, n_neighbors=15)\n",
        "sc.tl.umap(protein_adata)\n",
        "sc.pl.umap(protein_adata, color='celltype')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cf9649-d116-4014-abe6-12686cf91e6f",
      "metadata": {
        "id": "52cf9649-d116-4014-abe6-12686cf91e6f"
      },
      "outputs": [],
      "source": [
        "# make sure no feature is static\n",
        "rna_active = rna_adata.X\n",
        "protein_active = protein_adata.X\n",
        "rna_active = rna_active[:, rna_active.std(axis=0) > 1e-5] # these are fine since already using variable features\n",
        "protein_active = protein_active[:, protein_active.std(axis=0) > 1e-5] # protein are generally variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcdc89ae-a320-4ede-91e3-5c48275e6ba9",
      "metadata": {
        "id": "fcdc89ae-a320-4ede-91e3-5c48275e6ba9"
      },
      "outputs": [],
      "source": [
        "# inspect shape of the four matrices\n",
        "print(rna_active.shape)\n",
        "print(protein_active.shape)\n",
        "print(rna_shared.shape)\n",
        "print(protein_shared.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d10d81-0055-416d-8d09-a924e3d8d06b",
      "metadata": {
        "id": "b9d10d81-0055-416d-8d09-a924e3d8d06b"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(rna_active).to_csv('scratch/results/rna_active.csv', index=False)\n",
        "pd.DataFrame(protein_active).to_csv('scratch/results/protein_active.csv', index=False)\n",
        "pd.DataFrame(rna_shared).to_csv('scratch/results/rna_shared.csv', index=False)\n",
        "pd.DataFrame(protein_shared).to_csv('scratch/results/protein_shared.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba00b4c0-231e-40db-9ed5-6a7dcc197ec0",
      "metadata": {
        "id": "ba00b4c0-231e-40db-9ed5-6a7dcc197ec0"
      },
      "source": [
        "## Fitting MaxFuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e048ec2a-2959-4ff6-a837-ed254260934b",
      "metadata": {
        "id": "e048ec2a-2959-4ff6-a837-ed254260934b"
      },
      "outputs": [],
      "source": [
        "rna_active = pd.read_csv('scratch/results/rna_active.csv').values\n",
        "protein_active = pd.read_csv('scratch/results/protein_active.csv').values\n",
        "rna_shared = pd.read_csv('scratch/results/rna_shared.csv').values\n",
        "protein_shared = pd.read_csv('scratch/results/protein_shared.csv').values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbce4d0-f6a6-4640-8fe9-42f1d4a63b47",
      "metadata": {
        "id": "1cbce4d0-f6a6-4640-8fe9-42f1d4a63b47"
      },
      "source": [
        "### Step I: preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26426c39-57a0-428e-b947-e7ed7de9010e",
      "metadata": {
        "id": "26426c39-57a0-428e-b947-e7ed7de9010e"
      },
      "outputs": [],
      "source": [
        "# call constructor for Fusor object\n",
        "# which is the main object for running MaxFuse pipeline\n",
        "\n",
        "# flipping arr1 and arr2 for downstream r-l match_cells\n",
        "fusor = mf.model.Fusor(\n",
        "    shared_arr1=rna_shared,\n",
        "    shared_arr2=protein_shared,\n",
        "    active_arr1=rna_active,\n",
        "    active_arr2=protein_active,\n",
        "    labels1=None,\n",
        "    labels2=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c18fe9-8d22-4f60-93dc-4cac51ded84e",
      "metadata": {
        "id": "c1c18fe9-8d22-4f60-93dc-4cac51ded84e"
      },
      "outputs": [],
      "source": [
        "fusor.split_into_batches(\n",
        "    max_outward_size=5000,\n",
        "    matching_ratio=3,\n",
        "    metacell_size=2,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1105d241-2419-4573-b133-8ca8d1609799",
      "metadata": {
        "id": "1105d241-2419-4573-b133-8ca8d1609799"
      },
      "outputs": [],
      "source": [
        "# plot top singular values of active_arr1 on a random batch\n",
        "fusor.plot_singular_values(\n",
        "    target='active_arr1',\n",
        "    n_components=None # can also explicitly specify the number of components\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d04e68-e8c6-4e48-8899-b193de3fe13f",
      "metadata": {
        "id": "88d04e68-e8c6-4e48-8899-b193de3fe13f"
      },
      "outputs": [],
      "source": [
        "# plot top singular values of active_arr2 on a random batch\n",
        "fusor.plot_singular_values(\n",
        "    target='active_arr2',\n",
        "    n_components=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309f9da2-bbd1-4985-b41b-563bd50097a4",
      "metadata": {
        "id": "309f9da2-bbd1-4985-b41b-563bd50097a4"
      },
      "outputs": [],
      "source": [
        "fusor.construct_graphs(\n",
        "    n_neighbors1=15,\n",
        "    n_neighbors2=15,\n",
        "    svd_components1=40,\n",
        "    svd_components2=20,\n",
        "    resolution1=2,\n",
        "    resolution2=2,\n",
        "    # if two resolutions differ less than resolution_tol\n",
        "    # then we do not distinguish between then\n",
        "    resolution_tol=0.1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2f3814-0014-43e6-a6a6-6352fc753672",
      "metadata": {
        "id": "2b2f3814-0014-43e6-a6a6-6352fc753672"
      },
      "source": [
        "### Step II: finding initial pivots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369f7a84-8e03-4916-8be2-08acb5397d77",
      "metadata": {
        "id": "369f7a84-8e03-4916-8be2-08acb5397d77"
      },
      "outputs": [],
      "source": [
        "# plot top singular values of shared_arr1 on a random batch\n",
        "fusor.plot_singular_values(\n",
        "    target='shared_arr1',\n",
        "    n_components=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d92d26-1606-4331-a0d7-973f6b9e0a08",
      "metadata": {
        "id": "62d92d26-1606-4331-a0d7-973f6b9e0a08"
      },
      "outputs": [],
      "source": [
        "# plot top singular values of shared_arr2 on a random batch\n",
        "fusor.plot_singular_values(\n",
        "    target='shared_arr2',\n",
        "    n_components=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d439e96-89d1-4566-a2bb-9acbabc4e715",
      "metadata": {
        "id": "2d439e96-89d1-4566-a2bb-9acbabc4e715"
      },
      "outputs": [],
      "source": [
        "# load var_names and cell type labels here\n",
        "# retained_var_names_df = pd.read_csv('scratch/results/retained_rna_names.csv')\n",
        "# retained_rna_var_names = var_names_df['Gene Names'].values\n",
        "\n",
        "var_names_df = pd.read_csv('scratch/results/var_names.csv')\n",
        "rna_var_names = var_names_df['Gene Names'].values\n",
        "\n",
        "labels_rna = pd.read_csv('scratch/results/labels_rna.csv', index_col=0)\n",
        "labels_rna = labels_rna.values\n",
        "\n",
        "# load the nearest_neighbors from the pickle file\n",
        "with open('scratch/nearest_neighbors.pkl', 'rb') as file:\n",
        "    nearest_neighbors = pickle.load(file)\n",
        "\n",
        "# load interaction data\n",
        "file_path = 'scratch/rl_interactions/filtered_interaction_scores.csv'\n",
        "interaction_df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01c238f-09d3-4c89-b130-b7d3d7062067",
      "metadata": {
        "id": "a01c238f-09d3-4c89-b130-b7d3d7062067"
      },
      "outputs": [],
      "source": [
        "retained_names_set = set(rna_var_names)\n",
        "\n",
        "gene_a_set = set(interaction_df['gene_b'])\n",
        "gene_b_set = set(interaction_df['gene_a'])\n",
        "\n",
        "gene_pairs_set = set(zip(interaction_df['gene_b'], interaction_df['gene_a']))\n",
        "common_pairs = {pair for pair in gene_pairs_set if pair[0] in retained_names_set and pair[1] in retained_names_set}\n",
        "count_common_pairs = len(common_pairs)\n",
        "\n",
        "print(f\"Number of (gene_a, gene_b) pairs where both genes are in rna_var_names: {count_common_pairs}\")\n",
        "print(\"Common pairs:\", common_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794a1d8f-e026-4b12-bbcc-b79603fac232",
      "metadata": {
        "id": "794a1d8f-e026-4b12-bbcc-b79603fac232"
      },
      "outputs": [],
      "source": [
        "def find_init_pivots(\n",
        "        self,\n",
        "        wt1=0.3, wt2=0.3,\n",
        "        svd_components1=None, svd_components2=None,\n",
        "        randomized_svd=False, svd_runs=1,\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform initial matching.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    wt1: float, default=0.3\n",
        "        The shrinkage weight to put on the raw data for arr1.\n",
        "    wt2: float, default=0.3\n",
        "        The shrinkage weight to put on the raw data for arr2.\n",
        "    svd_components1: None or int, default=None\n",
        "        If not None, perform SVD to reduce the dimension of self.shared_arr1.\n",
        "    svd_components2: None or int, default=None\n",
        "        If not None, perform SVD to reduce the dimension of self.shared_arr2.\n",
        "    randomized_svd: bool, default=False\n",
        "        Whether to use randomized SVD.\n",
        "    svd_runs: int, default=1\n",
        "        Perform multiple runs of SVD and the one with lowest Frobenious reconstruction error is selected.\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    self._init_matching = []\n",
        "    self.distance_matrices = {}  # dictionary to store distance matrices\n",
        "    for b1, b2 in self._batch1_to_batch2:\n",
        "        if verbose:\n",
        "            print(\n",
        "                'Now at batch {}<->{}...'.format(b1, b2),\n",
        "                flush=True\n",
        "            )\n",
        "        if self.metacell_size > 1:\n",
        "            arr1 = utils.get_centroids(\n",
        "                arr=self.shared_arr1[self._batch_to_indices1[b1], :],\n",
        "                labels=self._metacell_labels1[b1]\n",
        "            )\n",
        "        else:\n",
        "            arr1 = self.shared_arr1[self._batch_to_indices1[b1], :]\n",
        "\n",
        "        arr2 = self.shared_arr2[self._batch_to_indices2[b2], :]\n",
        "\n",
        "        edges1, edges2, clust_labels1, clust_labels2 = None, None, None, None\n",
        "        if self.method == 'centroid_shrinkage':\n",
        "            clust_labels1 = self._labels1[b1]\n",
        "            clust_labels2 = self._labels2[b2]\n",
        "        elif self.method == 'graph_smoothing':\n",
        "            edges1 = self._edges1[b1]\n",
        "            edges2 = self._edges2[b2]\n",
        "        else:\n",
        "            raise ValueError('self.method must be one of \\'centroid_shrinkage\\' or \\'graph_smoothing\\'.')\n",
        "\n",
        "        dist_matrix, res = get_init_matching(\n",
        "                arr1=arr1,\n",
        "                arr2=arr2,\n",
        "                clust_labels1=clust_labels1,\n",
        "                clust_labels2=clust_labels2,\n",
        "                edges1=edges1,\n",
        "                edges2=edges2,\n",
        "                wt1=wt1,\n",
        "                wt2=wt2,\n",
        "                randomized_svd=randomized_svd,\n",
        "                svd_runs=svd_runs,\n",
        "                svd_components1=svd_components1,\n",
        "                svd_components2=svd_components2,\n",
        "                verbose=False\n",
        "            )\n",
        "        self.distance_matrices[(b1, b2)] = dist_matrix  # save the distance matrix\n",
        "        self._init_matching.append(res)\n",
        "\n",
        "    if verbose:\n",
        "        print('Done!', flush=True)\n",
        "\n",
        "def get_init_matching(\n",
        "        arr1, arr2,\n",
        "        clust_labels1=None, clust_labels2=None,\n",
        "        edges1=None, edges2=None,\n",
        "        wt1=0.3, wt2=0.3,\n",
        "        randomized_svd=True,\n",
        "        svd_runs=1,\n",
        "        svd_components1=None, svd_components2=None,\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Assume the features of arr1 and arr2 are column-wise directly comparable,\n",
        "    obtain a matching by minimizing the correlation distance between arr1 and arr2.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    arr1: np.array of shape (n_samples1, n_features1)\n",
        "        The first data matrix.\n",
        "    arr2: np.array of shape (n_samples2, n_features2)\n",
        "        The second data matrix.\n",
        "    clust_labels1: None or np.array of shape (n_samples1, )\n",
        "        If not None, then it is the clustering label of the first data matrix,\n",
        "        and the smoothing of this matrix will be done via cluster centroid shrinkage.\n",
        "    clust_labels2: None or np.array of shape (n_samples2, )\n",
        "        Same as clust_labels1 but for the second data matrix.\n",
        "    edges1: None or list of length two or three\n",
        "        If not None, then each edge in the graph is (edges[0][i], edges[1][i]) with weight edges[2][i] (if exists)\n",
        "        and the smoothing of this matrix will be done via graph smoothing.\n",
        "    edges2: None or scipy.sparse.csr_matrix of shape (n_samples2, n_samples2)\n",
        "        Same as edges1 but for the second data matrix.\n",
        "    wt1: float, default=0.3\n",
        "        The smoothing of the first data matrix will be wt1 * arr1 + (1-wt1) * shrinkage_targets,\n",
        "        where the shrinkage_targets will be either the cluster centroids or the average of graph neighbors.\n",
        "    wt2: float, default=0.3\n",
        "        Same as wt1 but for the second data matrix.\n",
        "    randomized_svd: bool, default=False\n",
        "        Whether to use randomized svd.\n",
        "    svd_runs: int, default=1\n",
        "        Randomized SVD will result in different runs,\n",
        "        so if randomized_svd=True, perform svd_runs many randomized SVDs, and pick the one with the\n",
        "        smallest Frobenious reconstruction error.\n",
        "        If randomized_svd=False, svd_runs is forced to be 1.\n",
        "    svd_components1: None or int\n",
        "        If None, then do not do SVD,\n",
        "        else, number of components to keep when doing SVD de-noising for the first data matrix.\n",
        "    svd_components2: None or int\n",
        "        Same as svd_components1 but for the second data matrix.\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matching: list of length 3\n",
        "        rows, cols, vals = matching,\n",
        "        Each matched pair is rows[i], cols[i], their distance is vals[i].\n",
        "    \"\"\"\n",
        "    assert arr1.shape[1] == arr2.shape[1]\n",
        "    # labels and edges cannot be specified simultaneously\n",
        "    assert (clust_labels1 is None) or (edges1 is None)\n",
        "    assert (clust_labels2 is None) or (edges2 is None)\n",
        "\n",
        "    arr1, arr2 = utils.drop_zero_variability_columns(arr_lst=[arr1, arr2])\n",
        "\n",
        "    # smoothing and denoising\n",
        "    if verbose:\n",
        "        print(\"Denoising the data...\", flush=True)\n",
        "\n",
        "    if clust_labels1 is not None:\n",
        "        arr1 = utils.shrink_towards_centroids(arr=arr1, labels=clust_labels1, wt=wt1)\n",
        "    elif edges1 is not None:\n",
        "        arr1 = utils.graph_smoothing(arr=arr1, edges=edges1, wt=wt1)\n",
        "    arr1 = utils.svd_denoise(\n",
        "        arr=arr1, n_components=svd_components1, randomized=randomized_svd,\n",
        "        n_runs=svd_runs\n",
        "    )\n",
        "\n",
        "    if clust_labels2 is not None:\n",
        "        arr2 = utils.shrink_towards_centroids(arr=arr2, labels=clust_labels2, wt=wt2)\n",
        "    elif edges2 is not None:\n",
        "        arr2 = utils.graph_smoothing(arr=arr2, edges=edges2, wt=wt2)\n",
        "    arr2 = utils.svd_denoise(\n",
        "        arr=arr2, n_components=svd_components2, randomized=randomized_svd,\n",
        "        n_runs=svd_runs\n",
        "    )\n",
        "\n",
        "    dist, res = match_init_cells(arr1=arr1, arr2=arr2, verbose=verbose)\n",
        "    if verbose:\n",
        "        print('Initial matching completed!', flush=True)\n",
        "\n",
        "    return dist, res\n",
        "\n",
        "def match_init_cells(arr1, arr2, base_dist=None, wt_on_base_dist=0, verbose=True):\n",
        "    \"\"\"\n",
        "    Get matching between arr1 and arr2 using linear assignment, the distance is 1 - Pearson correlation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    arr1: np.array of shape (n_samples1, n_features)\n",
        "        The first data matrix\n",
        "    arr2: np.array of shape (n_samples2, n_features)\n",
        "        The second data matrix\n",
        "    base_dist: None or np.ndarray of shape (n_samples1, n_samples2)\n",
        "        Baseline distance matrix\n",
        "    wt_on_base_dist: float between 0 and 1\n",
        "        The final distance matrix to use is (1-wt_on_base_dist) * dist[arr1, arr2] + wt_on_base_dist * base_dist\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rows, cols, vals: list\n",
        "        Each matched pair of rows[i], cols[i], their distance is vals[i]\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print('Start the matching process...', flush=True)\n",
        "        print('Computing the distance matrix...', flush=True)\n",
        "    dist = utils.cdist_correlation(arr1, arr2)\n",
        "    if base_dist is not None:\n",
        "        if verbose:\n",
        "            print(\n",
        "                f'Interpolating {1-wt_on_base_dist} * dist[arr1, arr2] + {wt_on_base_dist} * base_dist...',\n",
        "                flush=True\n",
        "            )\n",
        "        dist = (1-wt_on_base_dist) * dist + wt_on_base_dist * base_dist\n",
        "    if verbose:\n",
        "        print('Solving linear assignment...', flush=True)\n",
        "    rows, cols = linear_sum_assignment(dist)\n",
        "    if verbose:\n",
        "        print('Linear assignment completed!', flush=True)\n",
        "\n",
        "    matching = rows, cols, np.array([dist[i, j] for i, j in zip(rows, cols)])\n",
        "\n",
        "    return dist, matching\n",
        "\n",
        "Fusor.find_initial_pivots = find_init_pivots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd4509cf-dbb5-4f00-85f4-3700e32f1402",
      "metadata": {
        "id": "fd4509cf-dbb5-4f00-85f4-3700e32f1402"
      },
      "outputs": [],
      "source": [
        "# based off normal dist values\n",
        "fusor.find_initial_pivots(\n",
        "    wt1=0.3, wt2=0.3,\n",
        "    svd_components1=15, svd_components2=20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33671389-493c-4ccf-b463-ecc46aedb9cd",
      "metadata": {
        "id": "33671389-493c-4ccf-b463-ecc46aedb9cd"
      },
      "source": [
        "### Step III: finding refined pivots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62068eed-7572-408d-b0a8-faf7488192d7",
      "metadata": {
        "id": "62068eed-7572-408d-b0a8-faf7488192d7"
      },
      "outputs": [],
      "source": [
        "# plot top canonical correlations in a random batch\n",
        "fusor.plot_canonical_correlations(\n",
        "    svd_components1=50,\n",
        "    svd_components2=None,\n",
        "    cca_components=45\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6389f9-fb15-4437-bd3a-0e47527ff4f5",
      "metadata": {
        "id": "9b6389f9-fb15-4437-bd3a-0e47527ff4f5"
      },
      "outputs": [],
      "source": [
        "# precompute scores for all interaction types in a vectorized manner\n",
        "def precompute_interaction_scores(interaction_df):\n",
        "    # interaction scores start from the 14th column\n",
        "    interaction_columns = interaction_df.columns[13:]  # adjust index as needed\n",
        "\n",
        "    # dictionary to store total scores for each interaction type\n",
        "    interaction_scores = {}\n",
        "\n",
        "    for column in interaction_columns:\n",
        "        # sum all positive scores directly\n",
        "        total_score = interaction_df[column][interaction_df[column] > 0].sum()\n",
        "        interaction_scores[column] = total_score\n",
        "    scores_df = pd.DataFrame(list(interaction_scores.items()), columns=['Interaction_Type', 'Total_Score'])\n",
        "\n",
        "    return scores_df\n",
        "\n",
        "interaction_scores_df = precompute_interaction_scores(interaction_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87274465-7d1e-487a-8555-e289db0c0006",
      "metadata": {
        "id": "87274465-7d1e-487a-8555-e289db0c0006"
      },
      "outputs": [],
      "source": [
        "def compute_interaction_score(index1, index2, interaction_scores_df, labels_rna):\n",
        "    sc_cell_type = labels_rna[index1][0]\n",
        "    matched_sc_cell_type = labels_rna[index2][0]\n",
        "\n",
        "    cell_type_pair = f\"{sc_cell_type}|{matched_sc_cell_type}\"\n",
        "    reversed_cell_type_pair = f\"{matched_sc_cell_type}|{sc_cell_type}\"\n",
        "\n",
        "    # look up the scores from the precomputed DataFrame\n",
        "    score_normal = interaction_scores_df.loc[interaction_scores_df['Interaction_Type'] == cell_type_pair, 'Total_Score'].iloc[0]\n",
        "    score_reversed = interaction_scores_df.loc[interaction_scores_df['Interaction_Type'] == reversed_cell_type_pair, 'Total_Score'].iloc[0]\n",
        "\n",
        "    return max(score_normal, score_reversed)\n",
        "\n",
        "def precompute_indices(arr, rna_active):\n",
        "    # compute cosine similarity for the entire array at once\n",
        "    similarities = cosine_similarity(arr, rna_active)\n",
        "    indices = np.argmax(similarities, axis=1)  # get indices of max similarity for each cell in arr\n",
        "    return indices\n",
        "\n",
        "def adjust_distance_matrix(arr1, dist, initial_cols, interaction_scores_df, labels_rna, nearest_neighbors, rl_factor=0.1):\n",
        "    # precompute indices for arr1 against rna_active\n",
        "    indices_arr1 = precompute_indices(arr1, rna_active)\n",
        "\n",
        "    # initialize a copy of the distance matrix for adjustments\n",
        "    adjusted_dist = dist.copy()\n",
        "\n",
        "    # iterate through each cell in arr1\n",
        "    for i in range(arr1.shape[0]):\n",
        "        closest_sp_sc_indices = np.argsort(dist[i])[:100]  # get indices of the closest 50 cells for each cell in arr1\n",
        "\n",
        "        interaction_scores = []\n",
        "        for j in closest_sp_sc_indices:\n",
        "            closest_sp_indices = nearest_neighbors[j]\n",
        "            for sp_index in closest_sp_indices:\n",
        "                matched_indices = np.where(initial_cols == sp_index)[0]\n",
        "                if matched_indices.size > 0:\n",
        "                    matched_sc_index = matched_indices[0]  # just use the first match.\n",
        "                    interaction_score = compute_interaction_score(indices_arr1[i], indices_arr1[matched_sc_index], interaction_scores_df, labels_rna)\n",
        "                    interaction_scores.append(interaction_score)\n",
        "\n",
        "            # calculate mean interaction score if any scores were calculated\n",
        "            mean_interaction_score = np.mean(interaction_scores) if interaction_scores else 0\n",
        "            if mean_interaction_score > 0:\n",
        "                adjusted_dist[i, j] -= rl_factor * mean_interaction_score\n",
        "                # print(f'Adjusted dist score: {adjusted_dist[i, j]}')\n",
        "        # print(f'cell {i} done!')\n",
        "\n",
        "    return adjusted_dist\n",
        "\n",
        "def match_rl_cells(arr1, arr2, init_matching, batch_index1, batch_index2, base_dist=None, wt_on_base_dist=0, verbose=True):\n",
        "    \"\"\"\n",
        "    Get matching between arr1 and arr2 using linear assignment, the distance is 1 - Pearson correlation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    arr1: np.array of shape (n_samples1, n_features)\n",
        "        The first data matrix\n",
        "    arr2: np.array of shape (n_samples2, n_features)\n",
        "        The second data matrix\n",
        "    base_dist: None or np.ndarray of shape (n_samples1, n_samples2)\n",
        "        Baseline distance matrix\n",
        "    wt_on_base_dist: float between 0 and 1\n",
        "        The final distance matrix to use is (1-wt_on_base_dist) * dist[arr1, arr2] + wt_on_base_dist * base_dist\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rows, cols, vals: list\n",
        "        Each matched pair of rows[i], cols[i], their distance is vals[i]\n",
        "    \"\"\"\n",
        "    print(f'arr1 shape: {arr1.shape}')\n",
        "    print(f'arr2 shape: {arr2.shape}')\n",
        "\n",
        "    if verbose:\n",
        "        print('Start the matching process...', flush=True)\n",
        "        print('Computing the distance matrix...', flush=True)\n",
        "\n",
        "    dist = fusor.distance_matrices[(batch_index1, batch_index2)]\n",
        "\n",
        "    # dist = utils.cdist_correlation(arr1, arr2)\n",
        "\n",
        "    # np.save('scratch/results/correlation_distance_matrix.npy', dist)\n",
        "\n",
        "    # dist = np.load('scratch/results/correlation_distance_matrix.npy')\n",
        "\n",
        "    if base_dist is not None:\n",
        "        if verbose:\n",
        "            print(f'Interpolating {1-wt_on_base_dist} * dist[arr1, arr2] + {wt_on_base_dist} * base_dist...', flush=True)\n",
        "        dist = (1-wt_on_base_dist) * dist + wt_on_base_dist * base_dist\n",
        "\n",
        "    if verbose:\n",
        "        print('Solving initial linear assignment...', flush=True)\n",
        "\n",
        "    print(f'dist shape: {dist.shape}')\n",
        "    print(f'dist : {dist}')\n",
        "\n",
        "    # Perform initial matching\n",
        "    # initial_rows, initial_cols = linear_sum_assignment(dist)\n",
        "    initial_rows, initial_cols = init_matching[0], init_matching[1]\n",
        "\n",
        "    # np.save('scratch/results/initial_rows.npy', initial_rows)\n",
        "    # np.save('scratch/results/initial_cols.npy', initial_cols)\n",
        "\n",
        "    # initial_rows = np.load('scratch/results/initial_rows.npy')\n",
        "    # initial_cols = np.load('scratch/results/initial_cols.npy')\n",
        "\n",
        "    print(f'initial rows shape: {initial_rows.shape}')\n",
        "    print(f'initial_rows: {initial_rows}')\n",
        "    print(f'initial cols shape: {initial_cols.shape}')\n",
        "    print(f'initial_cols: {initial_cols}')\n",
        "\n",
        "    if verbose:\n",
        "        print('Initial linear assignment completed!', flush=True)\n",
        "\n",
        "    if verbose:\n",
        "        print('Adjusting distance matrix based on receptor/ligand interactions...', flush=True)\n",
        "\n",
        "    # Adjust the distance matrix based on receptor/ligand interactions\n",
        "    dist = adjust_distance_matrix(arr1, dist, initial_cols, interaction_scores_df, labels_rna, nearest_neighbors)\n",
        "\n",
        "    print(f'new dist shape: {dist.shape}')\n",
        "    print(f'new dist : {dist}')\n",
        "\n",
        "    if verbose:\n",
        "        print('Solving adjusted linear assignment...', flush=True)\n",
        "\n",
        "    # Perform adjusted matching\n",
        "    adjusted_rows, adjusted_cols = linear_sum_assignment(dist)\n",
        "\n",
        "    if verbose:\n",
        "        print('Adjusted linear assignment completed!', flush=True)\n",
        "\n",
        "    return adjusted_rows, adjusted_cols, np.array([dist[i, j] for i, j in zip(adjusted_rows, adjusted_cols)])\n",
        "\n",
        "# match_utils.match_cells = match_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47861638-688a-49b1-8fde-6c08fa9ca4c8",
      "metadata": {
        "id": "47861638-688a-49b1-8fde-6c08fa9ca4c8"
      },
      "outputs": [],
      "source": [
        "def ref_pivots(\n",
        "        self,\n",
        "        wt1=0.5, wt2=0.5,\n",
        "        svd_components1=None, svd_components2=None,\n",
        "        cca_components=None,\n",
        "        filter_prop=0,\n",
        "        n_iters=1,\n",
        "        randomized_svd=False, svd_runs=1,\n",
        "        cca_max_iter=2000,\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform refined matching.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    wt1: float, default=0.3\n",
        "        The shrinkage weight to put on the raw data for arr1.\n",
        "    wt2: float, default=0.3\n",
        "        The shrinkage weight to put on the raw data for arr2.\n",
        "    svd_components1: None or int, default=None\n",
        "        If not None, perform SVD to reduce the dimension of self.active_arr1 before feeding it to CCA.\n",
        "    svd_components2: None or int, default=None\n",
        "        If not None, perform SVD to reduce the dimension of self.active_arr2 before feeding it to CCA.\n",
        "    cca_components: None or int, default=None\n",
        "        Number of CCA components.\n",
        "        If None, it is set to 100 or self.active_arr1.shape[1] or self.active_arr2.shape[1], whichever is smaller.\n",
        "    filter_prop: float, default=0.\n",
        "        CCA is performed on top 1-filter_prop slice of the data on which the matched distances are smallest.\n",
        "    n_iters: int, default=1\n",
        "        Number of refinement iterations.\n",
        "    randomized_svd: bool, default=False\n",
        "        Whether to perform randomized SVD.\n",
        "    svd_runs: int, default=1\n",
        "        Perform multiple runs of SVD and the one with lowest Frobenious reconstruction error is selected.\n",
        "    cca_max_iter: int, default=2000\n",
        "        Maximum iteration number for CCA.\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # save cca parameters for later use\n",
        "    self._svd_components1_for_cca_embedding = svd_components1\n",
        "    self._svd_components2_for_cca_embedding = svd_components2\n",
        "    self._randomized_svd_for_cca_embedding = randomized_svd\n",
        "    self._svd_runs_for_cca_embedding = svd_runs\n",
        "    self._cca_components = cca_components\n",
        "    self._cca_max_iter = cca_max_iter\n",
        "\n",
        "    self._refined_matching = []\n",
        "    for batch_idx, (b1, b2) in enumerate(self._batch1_to_batch2):\n",
        "        if verbose:\n",
        "            print(\n",
        "                'Now at batch {}<->{}...'.format(b1, b2),\n",
        "                flush=True\n",
        "            )\n",
        "        arr1_init, arr2_init = None, None\n",
        "        if self.metacell_size > 1:\n",
        "            arr1 = utils.get_centroids(\n",
        "                arr=self.active_arr1[self._batch_to_indices1[b1], :],\n",
        "                labels=self._metacell_labels1[b1]\n",
        "            )\n",
        "        else:\n",
        "            arr1 = self.active_arr1[self._batch_to_indices1[b1], :]\n",
        "\n",
        "        arr2 = self.active_arr2[self._batch_to_indices2[b2], :]\n",
        "        arr2_init = self.shared_arr2[self._batch_to_indices2[b2], :]\n",
        "\n",
        "        edges1, edges2, clust_labels1, clust_labels2 = None, None, None, None\n",
        "        if self.method == 'centroid_shrinkage':\n",
        "            clust_labels1 = self._labels1[b1]\n",
        "            clust_labels2 = self._labels2[b2]\n",
        "        elif self.method == 'graph_smoothing':\n",
        "            edges1 = self._edges1[b1]\n",
        "            edges2 = self._edges2[b2]\n",
        "        else:\n",
        "            raise ValueError('self.method must be one of \\'centroid_shrinkage\\' or \\'graph_smoothing\\'.')\n",
        "\n",
        "        self._refined_matching.append(\n",
        "            get_ref_matching(\n",
        "                init_matching=self._init_matching[batch_idx],\n",
        "                arr1=arr1,\n",
        "                arr2=arr2,\n",
        "                batch_index1=b1,  # Pass batch index for arr1\n",
        "                batch_index2=b2,  # Pass batch index for arr2\n",
        "                randomized_svd=randomized_svd,\n",
        "                svd_runs=svd_runs,\n",
        "                svd_components1=svd_components1,\n",
        "                svd_components2=svd_components2,\n",
        "                clust_labels1=clust_labels1,\n",
        "                clust_labels2=clust_labels2,\n",
        "                edges1=edges1,\n",
        "                edges2=edges2,\n",
        "                wt1=wt1,\n",
        "                wt2=wt2,\n",
        "                n_iters=n_iters,\n",
        "                filter_prop=filter_prop,\n",
        "                cca_components=cca_components,\n",
        "                cca_max_iter=cca_max_iter,\n",
        "                verbose=False\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if verbose:\n",
        "        print('Done!', flush=True)\n",
        "\n",
        "def get_ref_matching(\n",
        "        init_matching, arr1, arr2,\n",
        "        batch_index1, batch_index2,\n",
        "        randomized_svd=False, svd_runs=1,\n",
        "        svd_components1=None, svd_components2=None,\n",
        "        clust_labels1=None, clust_labels2=None,\n",
        "        edges1=None, edges2=None,\n",
        "        wt1=0.5, wt2=0.5,\n",
        "        n_iters=3, filter_prop=0,\n",
        "        cca_components=15,\n",
        "        cca_max_iter=2000,\n",
        "        verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Refinement of init_matching.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    init_matching: list\n",
        "        init_matching[0][i], init_matching[1][i] is a matched pair,\n",
        "        and init_matching[2][i] is the distance for this pair.\n",
        "    arr1: np.array of shape (n_samples1, n_features1)\n",
        "        The first data matrix.\n",
        "    arr2: np.array of shape (n_samples2, n_features2)\n",
        "        The second data matrix.\n",
        "    randomized_svd: bool, default=False\n",
        "        Whether to use randomized SVD\n",
        "    svd_runs: int, default=1\n",
        "        Randomized SVD will result in different runs,\n",
        "        so if randomized_svd=True, perform svd_runs many randomized SVDs, and pick the one with the\n",
        "        smallest Frobenious reconstruction error.\n",
        "        If randomized_svd=False, svd_runs is forced to be 1.\n",
        "    svd_components1: None or int\n",
        "        If None, then do not do SVD,\n",
        "        else, number of components to keep when doing SVD de-noising for the first data matrix\n",
        "        before feeding into CCA.\n",
        "    svd_components2: None or int\n",
        "        Same as svd_components1 but for the second data matrix.\n",
        "    clust_labels1: None or np.array of shape (n_samples1, )\n",
        "        If not None, then it is the clustering label of the first data matrix,\n",
        "        and the smoothing of this matrix will be done via cluster centroid shrinkage.\n",
        "    clust_labels2: None or np.array of shape (n_samples2, )\n",
        "        Same as clust_labels1 but for the second data matrix.\n",
        "    edges1: None or list of length two or three\n",
        "        If not None, then each edge in the graph is (edges[0][i], edges[1][i]) with weight edges[2][i] (if exists)\n",
        "        and the smoothing of this matrix will be done via graph smoothing.\n",
        "    edges2: None or scipy.sparse.csr_matrix of shape (n_samples2, n_samples2)\n",
        "        Same as edges1 but for the second data matrix.\n",
        "    wt1: float, default=0.5\n",
        "        The smoothing of the first data matrix will be wt1 * (cca embedding of arr1) + (1-wt1) * shrinkage_targets,\n",
        "        where the shrinkage_targets will be either the cluster centroids or the average of graph neighbors.\n",
        "    wt2: float, default=0.5\n",
        "        Same as wt1 but for the second data matrix.\n",
        "    n_iters: int, default=3\n",
        "        Number of refinement iterations.\n",
        "    filter_prop: float, default=0\n",
        "        Proportion of matched pairs to discard before feeding into refinement iterations.\n",
        "    cca_components: int, default=15\n",
        "        Number of CCA components.\n",
        "    cca_max_iter: int, default=2000,\n",
        "        Maximum number of CCA iterations.\n",
        "    verbose: bool, default=True\n",
        "        Whether to print the progress.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matching: list of length 3\n",
        "        rows, cols, vals = matching,\n",
        "        Each matched pair is rows[i], cols[i], their distance is vals[i].\n",
        "    \"\"\"\n",
        "    ns = [len(x) for x in init_matching]\n",
        "    assert ns[0] == ns[1] == ns[2]\n",
        "    # labels and edges can not be specified simultaneously\n",
        "    assert (clust_labels1 is None) or (edges1 is None)\n",
        "    assert (clust_labels2 is None) or (edges2 is None)\n",
        "    assert isinstance(n_iters, int) and n_iters >= 1\n",
        "    assert 0 <= int(ns[0] * filter_prop) < ns[0]\n",
        "\n",
        "    assert 1 <= cca_components <= min(arr1.shape[1], arr2.shape[1])\n",
        "\n",
        "    # incorporate receptor-ligand interactions first!\n",
        "    if verbose:\n",
        "        print('Performing initial matching based on full feature set...', flush=True)\n",
        "\n",
        "    # Initial matching using all features\n",
        "    init_matching = match_rl_cells(arr1=arr1, arr2=arr2, init_matching=init_matching, batch_index1=batch_index1, batch_index2=batch_index2, verbose=verbose)\n",
        "\n",
        "    arr1 = utils.drop_zero_variability_columns(arr_lst=[arr1])[0]\n",
        "    arr2 = utils.drop_zero_variability_columns(arr_lst=[arr2])[0]\n",
        "\n",
        "    if verbose:\n",
        "        print('Normalizing and reducing the dimension of the data...', flush=True)\n",
        "    arr1_svd = utils.svd_embedding(\n",
        "        arr=arr1, n_components=svd_components1,\n",
        "        randomized=randomized_svd, n_runs=svd_runs\n",
        "    )\n",
        "    arr2_svd = utils.svd_embedding(\n",
        "        arr=arr2, n_components=svd_components2,\n",
        "        randomized=randomized_svd, n_runs=svd_runs\n",
        "    )\n",
        "\n",
        "    # prepare the distance matrix used in the initial matching\n",
        "    cca_matching = init_matching\n",
        "    # iterative refinement\n",
        "    for it in range(n_iters):\n",
        "        if verbose:\n",
        "            print('Now at iteration {}:'.format(it), flush=True)\n",
        "        cca_matching = match_utils.get_refined_matching_one_iter(\n",
        "            init_matching=cca_matching,\n",
        "            arr1=arr1_svd, arr2=arr2_svd,\n",
        "            clust_labels1=clust_labels1,\n",
        "            clust_labels2=clust_labels2, edges1=edges1, edges2=edges2,\n",
        "            wt1=wt1, wt2=wt2, filter_prop=filter_prop,\n",
        "            cca_components=cca_components, cca_max_iter=cca_max_iter, verbose=verbose\n",
        "        )\n",
        "\n",
        "    arr1_cca, arr2_cca, _ = utils.cca_embedding(\n",
        "        arr1=arr1_svd, arr2=arr2_svd,\n",
        "        init_matching=cca_matching,\n",
        "        filter_prop=filter_prop,\n",
        "        n_components=cca_components,\n",
        "        max_iter=cca_max_iter)\n",
        "\n",
        "    if verbose:\n",
        "        print('Refined matching completed!', flush=True)\n",
        "    return cca_matching\n",
        "\n",
        "Fusor.refine_pivots = ref_pivots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca95576-a7b6-4da5-b70d-e8bd71b4f42d",
      "metadata": {
        "id": "3ca95576-a7b6-4da5-b70d-e8bd71b4f42d"
      },
      "outputs": [],
      "source": [
        "# now will be adjusted r-l dist\n",
        "fusor.refine_pivots(\n",
        "    wt1=0.3, wt2=0.3,\n",
        "    svd_components1=40, svd_components2=None,\n",
        "    cca_components=30,\n",
        "    n_iters=1,\n",
        "    randomized_svd=False,\n",
        "    svd_runs=1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19738a0a-c0ee-4874-9fb4-6f31aba96801",
      "metadata": {
        "id": "19738a0a-c0ee-4874-9fb4-6f31aba96801"
      },
      "outputs": [],
      "source": [
        "fusor.filter_bad_matches(target='pivot', filter_prop=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19588de-5d54-4619-a56d-d2b33f105290",
      "metadata": {
        "id": "a19588de-5d54-4619-a56d-d2b33f105290"
      },
      "outputs": [],
      "source": [
        "pivot_matching = fusor.get_matching(order=(2, 1),target='pivot') # flipping order since flipped arr1 and arr2\n",
        "\n",
        "lv1_acc = mf.metrics.get_matching_acc(matching=pivot_matching,\n",
        "    labels1=labels_rna, # here too\n",
        "    labels2=labels_codex, # here too\n",
        "    order = (2,1) # here too\n",
        ")\n",
        "lv1_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604e6530-f346-4355-ada3-fcf23d88b74b",
      "metadata": {
        "id": "604e6530-f346-4355-ada3-fcf23d88b74b"
      },
      "outputs": [],
      "source": [
        "# We can inspect the first pivot pair.\n",
        "[pivot_matching[0][0], pivot_matching[1][0], pivot_matching[2][0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae48c73-3dc5-4459-baa9-6f101c0a8c21",
      "metadata": {
        "id": "aae48c73-3dc5-4459-baa9-6f101c0a8c21"
      },
      "source": [
        "### Step IV: propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1839d551-4695-4189-9899-1be97cb5ca33",
      "metadata": {
        "id": "1839d551-4695-4189-9899-1be97cb5ca33"
      },
      "outputs": [],
      "source": [
        "fusor.propagate(\n",
        "    svd_components1=40,\n",
        "    svd_components2=None,\n",
        "    wt1=0.7,\n",
        "    wt2=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3800045d-2c94-4779-8957-330bb546d349",
      "metadata": {
        "id": "3800045d-2c94-4779-8957-330bb546d349"
      },
      "outputs": [],
      "source": [
        "fusor.filter_bad_matches(\n",
        "    target='propagated',\n",
        "    filter_prop=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67bf94d7-59fe-430c-bd9a-9a8117648691",
      "metadata": {
        "id": "67bf94d7-59fe-430c-bd9a-9a8117648691"
      },
      "outputs": [],
      "source": [
        "full_matching = fusor.get_matching(order=(2, 1), target='full_data') # likewise flipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2855c67-5eed-4bf2-8b3a-3933f7f29908",
      "metadata": {
        "id": "f2855c67-5eed-4bf2-8b3a-3933f7f29908"
      },
      "outputs": [],
      "source": [
        "# compute the cell type level matching accuracy, for the full (filtered version) dataset\n",
        "lv1_acc = mf.metrics.get_matching_acc(matching=full_matching,\n",
        "    labels1=labels_rna, # here too\n",
        "    labels2=labels_codex # here too\n",
        ")\n",
        "lv1_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5785970b-5e0b-4679-827d-777354c295fe",
      "metadata": {
        "id": "5785970b-5e0b-4679-827d-777354c295fe"
      },
      "source": [
        "## Step V: downstream analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef2f03ff-00a8-4b2f-b5ee-80b671e06166",
      "metadata": {
        "id": "ef2f03ff-00a8-4b2f-b5ee-80b671e06166"
      },
      "outputs": [],
      "source": [
        "rna_cca, protein_cca_sub = fusor.get_embedding(\n",
        "    active_arr1=fusor.active_arr1,\n",
        "    active_arr2=fusor.active_arr2[full_matching[1],:] # cells in codex remained after filtering\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d664f1f7-0fe7-4e2b-8eb1-5a78daef3a7b",
      "metadata": {
        "id": "d664f1f7-0fe7-4e2b-8eb1-5a78daef3a7b"
      },
      "outputs": [],
      "source": [
        "labels_rna = labels_rna.ravel()  # this flattens `labels_rna` to 1D if it is 2D\n",
        "print(labels_rna.shape)\n",
        "print(labels_codex.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a536229-bffc-4ef7-8db5-0bf5d6bd2268",
      "metadata": {
        "id": "5a536229-bffc-4ef7-8db5-0bf5d6bd2268"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "subs = 13000\n",
        "randix = np.random.choice(protein_cca_sub.shape[0],subs, replace = False)\n",
        "\n",
        "dim_use = 15 # dimensions of the CCA embedding to be used for UMAP etc.\n",
        "\n",
        "cca_adata = ad.AnnData(\n",
        "    np.concatenate((rna_cca[:,:dim_use], protein_cca_sub[randix,:dim_use]), axis=0),\n",
        "    dtype=np.float32\n",
        ")\n",
        "cca_adata.obs['data_type'] = ['rna'] * rna_cca.shape[0] + ['protein'] * subs\n",
        "cca_adata.obs['cell_type'] = list(np.concatenate((\n",
        "    labels_rna, labels_codex[full_matching[1]][randix]), axis = 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6cd24c9-7ad9-408f-bd44-4881d53b5732",
      "metadata": {
        "id": "b6cd24c9-7ad9-408f-bd44-4881d53b5732"
      },
      "outputs": [],
      "source": [
        "sc.pp.neighbors(cca_adata, n_neighbors=15)\n",
        "sc.tl.umap(cca_adata)\n",
        "sc.pl.umap(cca_adata, color='data_type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3782d329-e974-4365-8b76-98af78d9519b",
      "metadata": {
        "id": "3782d329-e974-4365-8b76-98af78d9519b"
      },
      "outputs": [],
      "source": [
        "sc.pl.umap(cca_adata, color='cell_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eeb8c38-a1bc-4895-a1a5-511f275790e1",
      "metadata": {
        "id": "5eeb8c38-a1bc-4895-a1a5-511f275790e1"
      },
      "source": [
        "## Step VI: spatial profiling analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890b5a6d-6cc5-4057-afc6-bcebadb2efbc",
      "metadata": {
        "id": "890b5a6d-6cc5-4057-afc6-bcebadb2efbc"
      },
      "outputs": [],
      "source": [
        "# rna_adata.write(\"scratch/results/rna_adata.h5ad\")\n",
        "# protein_adata.write(\"scratch/results/protein_adata.h5ad\")\n",
        "# protein.to_csv('scratch/results/protein.csv', index=False)\n",
        "\n",
        "# custom JSON encoder that converts numpy int64 to int\n",
        "class JSONEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(JSONEncoder, self).default(obj)\n",
        "\n",
        "# writing the dictionary to a JSON file\n",
        "with open('scratch/results/full_matching_results_rl0.1_100.json', 'w') as jsonfile:\n",
        "    json.dump(full_matching, jsonfile, cls=JSONEncoder, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac81771-cf55-419f-8dff-17e27870d2d7",
      "metadata": {
        "id": "dac81771-cf55-419f-8dff-17e27870d2d7"
      },
      "outputs": [],
      "source": [
        "matching_df = pd.DataFrame(list(zip(full_matching[0], full_matching[1], full_matching[2])),\n",
        "                           columns=['mod1_indx', 'mod2_indx', 'score'])\n",
        "\n",
        "merged_df = matching_df.merge(protein, left_on='mod2_indx', right_index=True)\n",
        "\n",
        "\n",
        "plt.scatter(protein['X'], protein['Y'], c='blue', label='Spatial Proteomics', alpha=0.3, s=1)\n",
        "plt.scatter(merged_df['X'], merged_df['Y'], c='red', label='RNA (mod1)', alpha=0.3, s=1)\n",
        "\n",
        "plt.xlabel('Centroid X')\n",
        "plt.ylabel('Centroid Y')\n",
        "plt.title('RNA Mapped on Spatial Proteomics Data')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9fb0271-3f64-46a7-bc12-8af3946ed693",
      "metadata": {
        "id": "d9fb0271-3f64-46a7-bc12-8af3946ed693"
      },
      "outputs": [],
      "source": [
        "labels_rna_df = pd.DataFrame(labels_rna, columns=['cell_type'])\n",
        "\n",
        "merged_df = merged_df.merge(labels_rna_df, left_on='mod1_indx', right_index=True)\n",
        "\n",
        "plt.scatter(protein['X'], protein['Y'], c='blue', label='Spatial Proteomics', alpha=0.3, s=1)\n",
        "\n",
        "# define a color map for cell types\n",
        "cell_types = merged_df['cell_type'].unique()\n",
        "color_map = {cell_type: color for cell_type, color in zip(cell_types, plt.cm.get_cmap('tab10').colors)}\n",
        "\n",
        "# plot RNA data points based on cell types\n",
        "for cell_type in cell_types:\n",
        "    subset = merged_df[merged_df['cell_type'] == cell_type]\n",
        "    plt.scatter(subset['X'], subset['Y'], c=[color_map[cell_type]], label=f'RNA ({cell_type})', alpha=0.6, s=5)\n",
        "\n",
        "plt.xlabel('Centroid X')\n",
        "plt.ylabel('Centroid Y')\n",
        "plt.title('RNA Mapped on Spatial Proteomics Data')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01400cc5-aed7-4bb2-b60a-bef7b2ef6016",
      "metadata": {
        "id": "01400cc5-aed7-4bb2-b60a-bef7b2ef6016"
      },
      "outputs": [],
      "source": [
        "# create a confusion matrix comparing \"CELL_TYPE\" vs \"cell_type\"\n",
        "confusion_matrix = pd.crosstab(merged_df['CELL_TYPE'], merged_df['cell_type'])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='viridis', cbar=True,\n",
        "            xticklabels=True, yticklabels=True)\n",
        "plt.title('Confusion Matrix: CODEX (Ground-truth) vs scRNA-seq (Predicted)')\n",
        "plt.xlabel('scRNA-seq (Predicted)')\n",
        "plt.ylabel('CODEX (Ground)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.savefig('scratch/figures/rl_factor0.1_500.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b94ec25-97f4-4137-bd3b-aefb830a8875",
      "metadata": {
        "id": "4b94ec25-97f4-4137-bd3b-aefb830a8875"
      },
      "outputs": [],
      "source": [
        "df_copy = df.copy()\n",
        "\n",
        "# custom mappings for cell type categories\n",
        "cell_type_map = {\n",
        "    'Macrophages': 'Myeloid cells',\n",
        "    'Macrophages M2-like': 'Myeloid cells',\n",
        "    'Dendritic cells': 'Myeloid cells',\n",
        "    'Neutrophils': 'Myeloid cells',\n",
        "    'B cells': 'B cells',\n",
        "    'INFg+': 'B cells',\n",
        "    'Epithelium (INOS+)': 'Epithelium',\n",
        "    'Epithelium (INOS-)': 'Epithelium'\n",
        "}\n",
        "\n",
        "# apply mappings to the copy\n",
        "df_copy['CELL_TYPE'] = df_copy['CELL_TYPE'].replace(cell_type_map)\n",
        "df_copy['cell_type'] = df_copy['cell_type'].replace({\n",
        "    'Macrophages M2-like': 'Macrophages',\n",
        "    'Macrophages': 'Macrophages'\n",
        "})\n",
        "\n",
        "# remove 'Endothelial cells'\n",
        "filtered_df = df_copy[df_copy['CELL_TYPE'] != 'Endothelial cells']\n",
        "\n",
        "confusion_matrix = pd.crosstab(filtered_df['CELL_TYPE'], filtered_df['cell_type'])\n",
        "\n",
        "# order for plotting\n",
        "y_order = ['B cells', 'CD4 T cells', 'CD8 T cells', 'Fibroblasts', 'Myeloid cells', 'Epithelium']\n",
        "x_order = ['B cells', 'CD4 T cells', 'CD8 T cells', 'Fibroblasts', 'Macrophages', 'Malignant cells']\n",
        "\n",
        "# reorder the DataFrame for plotting\n",
        "confusion_matrix = confusion_matrix.reindex(index=y_order, columns=x_order).fillna(0)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='viridis', cbar=True,\n",
        "            xticklabels=True, yticklabels=True)\n",
        "plt.title('Confusion Matrix: CODEX (Ground-truth) vs scRNA-seq (Predicted)')\n",
        "plt.xlabel('scRNA-seq (Predicted)')\n",
        "plt.ylabel('CODEX (Ground)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd3ecd7-038e-40fd-90c7-551c45b33eb2",
      "metadata": {
        "id": "ffd3ecd7-038e-40fd-90c7-551c45b33eb2"
      },
      "outputs": [],
      "source": [
        "df_copy = filtered_df.copy()\n",
        "\n",
        "# custom mappings for cell type categories\n",
        "cell_type_map_y = {\n",
        "    'Epithelium (INOS+)': 'Malignant cells',\n",
        "    'Epithelium (INOS-)': 'Malignant cells'\n",
        "}\n",
        "\n",
        "cell_type_map_x = {\n",
        "    'Macrophages': 'Myeloid cells',\n",
        "    'Macrophages M2-like': 'Myeloid cells'\n",
        "}\n",
        "\n",
        "# apply mappings to the copy for y-axis (true labels) and x-axis (predicted labels)\n",
        "df_copy['CELL_TYPE'] = df_copy['CELL_TYPE'].replace(cell_type_map_y)\n",
        "df_copy['cell_type'] = df_copy['cell_type'].replace(cell_type_map_x)\n",
        "\n",
        "# extract labels for F1 and ARI calculation\n",
        "true_labels = df_copy['CELL_TYPE'].values  # adjusted true cell types\n",
        "predicted_labels = df_copy['cell_type'].values  # adjusted predicted cell types\n",
        "\n",
        "# calculate F1-score. Note: We use 'weighted' to account for label imbalance.\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "print(\"F1-score: {:.2f}\".format(f1))\n",
        "\n",
        "# calculate Adjusted Rand Index\n",
        "ari = adjusted_rand_score(true_labels, predicted_labels)\n",
        "print(\"Adjusted Rand Index (ARI): {:.2f}\".format(ari))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba13582-a865-4767-969d-7bf6f9569dcb",
      "metadata": {
        "id": "1ba13582-a865-4767-969d-7bf6f9569dcb"
      },
      "source": [
        "## Step VII: Pseudo-spot creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc1c2cf-73cd-4e9e-af88-950377a88b99",
      "metadata": {
        "id": "4cc1c2cf-73cd-4e9e-af88-950377a88b99"
      },
      "outputs": [],
      "source": [
        "### Visium Regions ###\n",
        "A = ad.read_h5ad('scratch/SP_SC_data/s4769_visium_data/visium_counts.h5ad')\n",
        "label_df = pd.read_csv('scratch/SP_SC_data/s4769_metadata.csv')\n",
        "\n",
        "def get_matching_codex_visium_dfs(target_acq):\n",
        "    # target_region = region_id_to_label[target_acq]\n",
        "    target_region = emdb.get_region_label_for_acquisition_id(target_acq)\n",
        "\n",
        "    # CODEX cell coordinates and cell types\n",
        "    # cell_type_df = pd.read_csv(os.path.join(root_path, 'raw_data', f'scratch/SP_SC_data/s4769_cell_types/{target_acq}.cell_types.csv'))\n",
        "    # cell_type_df.set_index('CELL_ID', inplace=True)\n",
        "    # cell_coord_df = pd.read_csv(os.path.join(root_path, 'raw_data', f'/{target_acq}.cell_data.csv'))\n",
        "    # cell_coord_df.set_index('CELL_ID', inplace=True)\n",
        "    # cell_data_df = cell_coord_df.join(cell_type_df)\n",
        "\n",
        "    # Visium spot coordinates and expression\n",
        "    xdim, ydim = emdb.get_image_dim_for_acquisition_id(target_acq)\n",
        "    spot_coord_df = pd.read_csv(f'scratch/SP_SC_data/s4769_visium_data/0{target_region}_tissue_positions_list.csv', index_col=0)\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['in_tissue'] == 1]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords x'] >= 0]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords x'] <= xdim]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords y'] >= 0]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords y'] <= ydim]\n",
        "    assert len(spot_coord_df) == len(set(spot_coord_df['barcode']))\n",
        "\n",
        "    patient_id = target_region.split('_')[0]\n",
        "    state = label_df[label_df['acq_id'] == target_acq]['diagnosis'].item().lower()\n",
        "    inds = []\n",
        "    valid_bcs = []\n",
        "\n",
        "    identifier_ar = list(A.obs.index)\n",
        "    for bc in spot_coord_df['barcode']:\n",
        "        assert len(bc) == 18\n",
        "        assert bc.endswith('-1')\n",
        "        identifier = f'cytassist_{patient_id}_{state}@{bc}'\n",
        "        if identifier in identifier_ar:\n",
        "            inds.append(identifier_ar.index(identifier))\n",
        "            valid_bcs.append(bc)\n",
        "        else:\n",
        "            print(\"Missing %s\" % identifier)\n",
        "\n",
        "    spot_coord_df = spot_coord_df.set_index('barcode').loc[valid_bcs].reset_index()\n",
        "    sub_A = A[inds]\n",
        "    assert sub_A.shape[0] == spot_coord_df.shape[0]\n",
        "\n",
        "    spot_ids = np.array(spot_coord_df['barcode'])\n",
        "    spot_coords = np.array(spot_coord_df[['transformed coords x', 'transformed coords y']])\n",
        "    spot_to_cells = {spot_id: [] for spot_id in spot_ids}\n",
        "    for cid, row in protein.iterrows():\n",
        "        x, y, ct = row['X'], row['Y'], row['CELL_TYPE']\n",
        "        dists_to_spots = np.linalg.norm(np.array([x, y]).reshape((1, 2)) - spot_coords, ord=2, axis=1)\n",
        "        if np.min(dists_to_spots) < 140:\n",
        "            matched_spot = spot_ids[np.argmin(dists_to_spots)]\n",
        "            spot_to_cells[matched_spot].append((cid, ct))\n",
        "\n",
        "    return sub_A, spot_coord_df, protein, spot_to_cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3912bfa9-d93a-4d25-a12f-c90654bda2b2",
      "metadata": {
        "id": "3912bfa9-d93a-4d25-a12f-c90654bda2b2"
      },
      "outputs": [],
      "source": [
        "# # Get the matching dataframes\n",
        "sub_A, spot_coord_df, protein, spot_to_cells = get_matching_codex_visium_dfs(target_acq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15e5c92-db9b-4675-a627-e8be5efd65f7",
      "metadata": {
        "id": "b15e5c92-db9b-4675-a627-e8be5efd65f7"
      },
      "outputs": [],
      "source": [
        "# convert the protein DataFrame to include only relevant rows\n",
        "protein_subset = protein[protein.index.isin([cell[0] for cells in spot_to_cells.values() for cell in cells])]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# plot Visium spots\n",
        "ax.scatter(spot_coord_df['transformed coords x'],\n",
        "           spot_coord_df['transformed coords y'],\n",
        "           c='blue', label='Visium Spots', s=50, alpha=0.6)\n",
        "\n",
        "# plot CODEX cells with colors indicating cell types\n",
        "cell_types = protein_subset['CELL_TYPE'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(cell_types))\n",
        "\n",
        "for i, cell_type in enumerate(cell_types):\n",
        "    cells_of_type = protein_subset[protein_subset['CELL_TYPE'] == cell_type]\n",
        "    ax.scatter(cells_of_type['X'], cells_of_type['Y'], c=[colors(i)], label=cell_type, s=20, alpha=0.6)\n",
        "\n",
        "ax.set_xlabel('X Coordinate')\n",
        "ax.set_ylabel('Y Coordinate')\n",
        "ax.set_title(f'CODEX Ground Truth Cells')\n",
        "\n",
        "ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0733b9c3-d7c6-4e3a-bff2-7dd419c5cc11",
      "metadata": {
        "id": "0733b9c3-d7c6-4e3a-bff2-7dd419c5cc11"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# fit and transform the coordinates for normalization\n",
        "spot_coord_df[['transformed coords x', 'transformed coords y']] = scaler.fit_transform(spot_coord_df[['transformed coords x', 'transformed coords y']])\n",
        "merged_df[['X', 'Y']] = scaler.fit_transform(merged_df[['X', 'Y']])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# plot Visium spots\n",
        "ax.scatter(spot_coord_df['transformed coords x'],\n",
        "           spot_coord_df['transformed coords y'],\n",
        "           c='blue', label='Visium Spots', s=50, alpha=0.6)\n",
        "\n",
        "# define a color map for cell types\n",
        "cell_types = merged_df['cell_type'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(cell_types))\n",
        "\n",
        "# plot RNA data points based on cell types\n",
        "for i, cell_type in enumerate(cell_types):\n",
        "    subset = merged_df[merged_df['cell_type'] == cell_type]\n",
        "    ax.scatter(subset['X'], subset['Y'], c=[colors(i)], label=f'RNA ({cell_type})', alpha=0.6, s=30)\n",
        "\n",
        "ax.set_xlabel('Normalized X Coordinate')\n",
        "ax.set_ylabel('Normalized Y Coordinate')\n",
        "ax.set_title('Matched scRNA-seq Cells')\n",
        "\n",
        "ax.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9570545d-d719-422d-b6a4-46d2727365d9",
      "metadata": {
        "id": "9570545d-d719-422d-b6a4-46d2727365d9"
      },
      "source": [
        "## Step VIII: Evaluating spatial accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd17789-2dc1-439c-a2aa-76c71ed6b0f0",
      "metadata": {
        "id": "8bd17789-2dc1-439c-a2aa-76c71ed6b0f0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# condensing the RNA to CODEX matched data onto its nearest visium spot\n",
        "\n",
        "# build a KD-tree for the Visium spots\n",
        "visium_coords = spot_coord_df[['transformed coords x', 'transformed coords y']].values\n",
        "kdtree = cKDTree(visium_coords)\n",
        "\n",
        "# find the nearest Visium spot for each RNA cell\n",
        "rna_coords = merged_df[['X', 'Y']].values\n",
        "distances, indices = kdtree.query(rna_coords)\n",
        "\n",
        "# add the nearest Visium spot index to the merged_df\n",
        "merged_df['nearest_spot_index'] = indices\n",
        "\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a5c4d7-d06e-49f0-9d06-9184c31314b9",
      "metadata": {
        "id": "15a5c4d7-d06e-49f0-9d06-9184c31314b9"
      },
      "outputs": [],
      "source": [
        "A = ad.read_h5ad('scratch/SP_SC_data/s4769_visium_data/visium_counts.h5ad')\n",
        "# label_df = pd.read_csv('scratch/SP_SC_data/s4769_metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2858684-e66f-42ce-b5d0-f5aa2d43ac23",
      "metadata": {
        "id": "e2858684-e66f-42ce-b5d0-f5aa2d43ac23"
      },
      "outputs": [],
      "source": [
        "indices_to_retain = merged_df['mod1_indx'].values\n",
        "\n",
        "# filter the scRNA_adata object to include only those cells\n",
        "filtered_rna_adata = rna_adata[indices_to_retain]\n",
        "\n",
        "# identify common genes\n",
        "common_genes = np.intersect1d(filtered_rna_adata.var_names, sub_A.var_names)\n",
        "\n",
        "len(common_genes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681b1f00-4cec-4d7d-92e2-71157bdb1f2e",
      "metadata": {
        "id": "681b1f00-4cec-4d7d-92e2-71157bdb1f2e"
      },
      "outputs": [],
      "source": [
        "# identify unique genes in scRNA and spatial transcriptomics datasets\n",
        "unique_genes_scRNA = np.setdiff1d(filtered_rna_adata.var_names, sub_A.var_names)\n",
        "unique_genes_spatial = np.setdiff1d(sub_A.var_names, filtered_rna_adata.var_names)\n",
        "\n",
        "# print the number of unique genes\n",
        "print(f\"Number of unique genes in scRNA: {len(unique_genes_scRNA)}\")\n",
        "print(f\"Number of unique genes in spatial transcriptomics: {len(unique_genes_spatial)}\")\n",
        "\n",
        "# print some of the unique genes to inspect potential naming issues\n",
        "print(\"Unique genes in scRNA (first 10):\", unique_genes_scRNA[:200])\n",
        "print(\"Unique genes in spatial transcriptomics (first 10):\", unique_genes_spatial[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c276a29-0d01-4913-b837-74a44a0936d6",
      "metadata": {
        "id": "4c276a29-0d01-4913-b837-74a44a0936d6"
      },
      "outputs": [],
      "source": [
        "# need to filter the rna_adata object include only selected ones\n",
        "indices_to_retain = merged_df['mod1_indx'].values\n",
        "filtered_rna_adata = rna_adata[indices_to_retain]\n",
        "\n",
        "common_genes = np.intersect1d(filtered_rna_adata.var_names, sub_A.var_names)\n",
        "filtered_rna_adata = filtered_rna_adata[:, common_genes]\n",
        "spatial_adata = sub_A[:, common_genes]\n",
        "\n",
        "# binarize the gene expression matrix (presence/absence of genes)\n",
        "scRNA_gene_presence = (filtered_rna_adata.X > 0).astype(int)\n",
        "scRNA_gene_presence_df = pd.DataFrame(scRNA_gene_presence, columns=filtered_rna_adata.var_names)\n",
        "\n",
        "# add nearest spot index to the gene presence DataFrame\n",
        "scRNA_gene_presence_df['nearest_spot_index'] = indices\n",
        "\n",
        "# aggregate RNA expressions based on the nearest Visium spot\n",
        "aggregated_data = scRNA_gene_presence_df.groupby('nearest_spot_index').max().reset_index()\n",
        "\n",
        "# compute the Jaccard index for each Visium spot\n",
        "def jaccard_index(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "# convert aggregated_data and ground_truth_df to sets of expressed genes\n",
        "ground_truth_gene_presence = (spatial_adata.X > 0).astype(int)\n",
        "ground_truth_df = pd.DataFrame(ground_truth_gene_presence.toarray(), columns=spatial_adata.var_names)\n",
        "\n",
        "aggregated_gene_sets = aggregated_data.drop('nearest_spot_index', axis=1).apply(lambda row: set(row[row > 0].index), axis=1)\n",
        "ground_truth_gene_sets = ground_truth_df.apply(lambda row: set(row[row > 0].index), axis=1)\n",
        "\n",
        "# initialize jaccard indices with zeros or NaNs for the length of spot_coord_df\n",
        "jaccard_indices = np.full(len(spot_coord_df), np.nan)\n",
        "\n",
        "# calculate Jaccard index for each spot\n",
        "for i in range(len(aggregated_gene_sets)):\n",
        "    spot_index = aggregated_data.loc[i, 'nearest_spot_index']\n",
        "    jaccard_indices[spot_index] = jaccard_index(aggregated_gene_sets[i], ground_truth_gene_sets.iloc[spot_index])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))]\n",
        "ax.scatter(spot_coord_df['transformed coords x'], spot_coord_df['transformed coords y'],\n",
        "           c='blue', label='Visium Spots', s=50, alpha=0.6)\n",
        "norm = plt.Normalize(0, 1)\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "# plot aggregated RNA data points based on Jaccard index\n",
        "scatter = ax.scatter(spot_coord_df['transformed coords x'], spot_coord_df['transformed coords y'],\n",
        "                     c=jaccard_indices, cmap=cmap, norm=norm, s=50, alpha=0.6)\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Jaccard Index')\n",
        "ax.set_xlabel('Normalized X Coordinate')\n",
        "ax.set_ylabel('Normalized Y Coordinate')\n",
        "ax.set_title('Jaccard Index of Aggregated RNA on Visium Spots')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9e1a12-c644-4e97-b549-28e9d1da8dbb",
      "metadata": {
        "id": "0c9e1a12-c644-4e97-b549-28e9d1da8dbb"
      },
      "outputs": [],
      "source": [
        "scRNA_gene_presence_df = pd.DataFrame(filtered_rna_adata.X, columns=filtered_rna_adata.var_names)\n",
        "\n",
        "# add nearest spot index to the gene presence DataFrame\n",
        "scRNA_gene_presence_df['nearest_spot_index'] = indices\n",
        "\n",
        "# aggregate RNA expressions based on nearest Visium spot\n",
        "aggregated_data = scRNA_gene_presence_df.groupby('nearest_spot_index').mean()\n",
        "\n",
        "# (spatial transcriptomics data)\n",
        "ground_truth_df = pd.DataFrame(spatial_adata.X.todense(), columns=spatial_adata.var_names)\n",
        "\n",
        "# initialize correlation indices with NaNs for length of spot_coord_df\n",
        "correlation_indices = np.full(len(spot_coord_df), np.nan)\n",
        "\n",
        "# calculate Spearman correlation for each spot\n",
        "for i in range(len(aggregated_data)):\n",
        "    spot_index = aggregated_data.index[i]\n",
        "    if spot_index < len(ground_truth_df):\n",
        "        correlation, _ = spearmanr(aggregated_data.iloc[i], ground_truth_df.iloc[spot_index])\n",
        "        correlation_indices[spot_index] = correlation\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "norm = plt.Normalize(np.nanmin(correlation_indices), np.nanmax(correlation_indices))\n",
        "cmap = plt.cm.viridis\n",
        "\n",
        "# plot aggregated RNA data points based on Spearman correlation\n",
        "scatter = ax.scatter(spot_coord_df['transformed coords x'], spot_coord_df['transformed coords y'],\n",
        "                     c=correlation_indices, cmap=cmap, norm=norm, s=50, alpha=0.6)\n",
        "\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Spearman Correlation')\n",
        "ax.set_xlabel('Normalized X Coordinate')\n",
        "ax.set_ylabel('Normalized Y Coordinate')\n",
        "ax.set_title('Spearman Correlation of Aggregated RNA on Visium Spots')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e90299-9389-4e32-92d8-9d11d9a5ea03",
      "metadata": {
        "id": "74e90299-9389-4e32-92d8-9d11d9a5ea03"
      },
      "outputs": [],
      "source": [
        "# plot Spearman correlation values as a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(correlation_indices, bins=30, kde=True)\n",
        "\n",
        "plt.title('Distribution of Spearman Correlation Values')\n",
        "plt.xlabel('Spearman Correlation')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad009dcc-523e-473f-acfc-a22f31e63035",
      "metadata": {
        "id": "ad009dcc-523e-473f-acfc-a22f31e63035"
      },
      "outputs": [],
      "source": [
        "# compute Spearman correlation for each gene across all spots\n",
        "ground_truth_df = pd.DataFrame(spatial_adata.X.toarray(), columns=spatial_adata.var_names)\n",
        "\n",
        "aggregated_data = aggregated_data.reindex(spot_coord_df.index)\n",
        "ground_truth_df = ground_truth_df.reindex(spot_coord_df.index)\n",
        "\n",
        "gene_correlations = []\n",
        "\n",
        "for gene in common_genes:\n",
        "    if gene in aggregated_data.columns and gene in ground_truth_df.columns:\n",
        "        # drop NaN values that may have resulted from the reindexing\n",
        "        valid_indices = ~np.isnan(aggregated_data[gene]) & ~np.isnan(ground_truth_df[gene])\n",
        "        gene_corr, _ = spearmanr(aggregated_data[gene][valid_indices], ground_truth_df[gene][valid_indices])\n",
        "        gene_correlations.append((gene, gene_corr))\n",
        "\n",
        "# sort genes by their correlation values\n",
        "gene_correlations_sorted = sorted(gene_correlations, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "gene_correlations_df = pd.DataFrame(gene_correlations_sorted, columns=['Gene', 'Spearman Correlation'])\n",
        "\n",
        "# plot the top 30 genes\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_n = 30\n",
        "sns.barplot(x='Spearman Correlation', y='Gene', data=gene_correlations_df.head(top_n))\n",
        "plt.title(f'Top {top_n} Genes by Spearman Correlation Across Modalities')\n",
        "plt.xlabel('Spearman Correlation')\n",
        "plt.ylabel('Gene')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c38dc0-b124-4191-8488-404b4bf76e51",
      "metadata": {
        "id": "85c38dc0-b124-4191-8488-404b4bf76e51"
      },
      "source": [
        "## Step IX: Cell-type Deconvolution Using Tangram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f668eff-06dc-4939-9caf-926240a52087",
      "metadata": {
        "id": "2f668eff-06dc-4939-9caf-926240a52087"
      },
      "outputs": [],
      "source": [
        "# raw adata for deconvolution\n",
        "scRNA_adata = ad.AnnData(\n",
        "    rna.tocsr(), dtype=np.float32\n",
        ")\n",
        "scRNA_adata.var_names = rna_names\n",
        "scRNA_adata.var_names_make_unique()\n",
        "scRNA_adata.obs['cell_type'] = rna_adata.obs['celltype']\n",
        "\n",
        "spatial_adata = sub_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3b722d-8984-4100-a097-ed863f9cd32f",
      "metadata": {
        "id": "4b3b722d-8984-4100-a097-ed863f9cd32f"
      },
      "outputs": [],
      "source": [
        "# loading raw Visium counts for Tangram comparison\n",
        "all_A = ad.read_h5ad('scratch/SP_SC_data/s4769_visium_data/visium_all.h5ad')\n",
        "label_df = pd.read_csv('scratch/SP_SC_data/s4769_metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d85a93-fb93-474a-9611-686126a61d19",
      "metadata": {
        "id": "15d85a93-fb93-474a-9611-686126a61d19"
      },
      "outputs": [],
      "source": [
        "def new_get_matching_codex_visium_dfs(target_acq):\n",
        "    # target_region = region_id_to_label[target_acq] # what is region_id_to_label\n",
        "    target_region = emdb.get_region_label_for_acquisition_id(target_acq)\n",
        "\n",
        "\n",
        "    # CODEX cell coordinates and cell types\n",
        "    # cell_type_df = pd.read_csv(os.path.join(root_path, 'raw_data', f'scratch/SP_SC_data/s4769_cell_types/{target_acq}.cell_types.csv'))\n",
        "    # cell_type_df.set_index('CELL_ID', inplace=True)\n",
        "    # cell_coord_df = pd.read_csv(os.path.join(root_path, 'raw_data', f'/{target_acq}.cell_data.csv'))\n",
        "    # cell_coord_df.set_index('CELL_ID', inplace=True)\n",
        "    # cell_data_df = cell_coord_df.join(cell_type_df)\n",
        "\n",
        "    # Visium spot coordinates and expression\n",
        "    xdim, ydim = emdb.get_image_dim_for_acquisition_id(target_acq)\n",
        "    spot_coord_df = pd.read_csv(f'scratch/SP_SC_data/s4769_visium_data/0{target_region}_tissue_positions_list.csv', index_col=0)\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['in_tissue'] == 1]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords x'] >= 0]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords x'] <= xdim]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords y'] >= 0]\n",
        "    spot_coord_df = spot_coord_df[spot_coord_df['transformed coords y'] <= ydim]\n",
        "    assert len(spot_coord_df) == len(set(spot_coord_df['barcode']))\n",
        "\n",
        "    patient_id = target_region.split('_')[0]\n",
        "    state = label_df[label_df['acq_id'] == target_acq]['diagnosis'].item().lower()\n",
        "    inds = []\n",
        "    valid_bcs = []\n",
        "\n",
        "    identifier_ar = list(all_A.obs.index)\n",
        "    for bc in spot_coord_df['barcode']:\n",
        "        assert len(bc) == 18\n",
        "        assert bc.endswith('-1')\n",
        "        identifier = f'cytassist_{patient_id}_{state}@{bc}'\n",
        "        if identifier in identifier_ar:\n",
        "            inds.append(identifier_ar.index(identifier))\n",
        "            valid_bcs.append(bc)\n",
        "        else:\n",
        "            print(\"Missing %s\" % identifier)\n",
        "\n",
        "    spot_coord_df = spot_coord_df.set_index('barcode').loc[valid_bcs].reset_index()\n",
        "    sub_all_A = all_A[inds]\n",
        "    assert sub_all_A.shape[0] == spot_coord_df.shape[0]\n",
        "\n",
        "    spot_ids = np.array(spot_coord_df['barcode'])\n",
        "    spot_coords = np.array(spot_coord_df[['transformed coords x', 'transformed coords y']])\n",
        "    spot_to_cells = {spot_id: [] for spot_id in spot_ids}\n",
        "    for cid, row in protein.iterrows():\n",
        "        x, y, ct = row['X'], row['Y'], row['CELL_TYPE']\n",
        "        dists_to_spots = np.linalg.norm(np.array([x, y]).reshape((1, 2)) - spot_coords, ord=2, axis=1)\n",
        "        if np.min(dists_to_spots) < 140:\n",
        "            matched_spot = spot_ids[np.argmin(dists_to_spots)]\n",
        "            spot_to_cells[matched_spot].append((cid, ct))\n",
        "\n",
        "    return sub_all_A, spot_coord_df, protein, spot_to_cells\n",
        "\n",
        "sub_all_A, new_spot_coord_df, protein, new_spot_to_cells = new_get_matching_codex_visium_dfs(target_acq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed21584-e976-4810-ad09-950dc6c6517e",
      "metadata": {
        "id": "0ed21584-e976-4810-ad09-950dc6c6517e"
      },
      "outputs": [],
      "source": [
        "# prepare data for Tangram\n",
        "tg.pp_adatas(scRNA_adata, spatial_adata, genes=None) # best not to normalize before this (use raw data).\n",
        "\n",
        "# map single-cell data to spatial data using Tangram\n",
        "ad_map = tg.map_cells_to_space(scRNA_adata, spatial_adata, mode='cells')\n",
        "\n",
        "# project cell types onto spatial data\n",
        "tg.project_cell_annotations(ad_map, spatial_adata)\n",
        "\n",
        "# extract deconvolution results\n",
        "proportions_df = spatial_adata.obsm['tangram_ct_pred']\n",
        "proportions_df = pd.DataFrame(proportions_df, index=spatial_adata.obs_names, columns=scRNA_adata.obs['cell_type'].unique())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(proportions_df, cmap='viridis', cbar_kws={'label': 'Proportion'})\n",
        "plt.title('Deconvolution of Spatial Transcriptomics Data')\n",
        "plt.xlabel('Cell Types')\n",
        "plt.ylabel('Spatial Spots')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd6d0ca-8996-4a18-b06c-0f0624e7955b",
      "metadata": {
        "id": "8cd6d0ca-8996-4a18-b06c-0f0624e7955b"
      },
      "outputs": [],
      "source": [
        "# extract the deconvolution results\n",
        "proportions_df = spatial_adata.obsm['tangram_ct_pred']\n",
        "proportions_df = pd.DataFrame(proportions_df, index=spatial_adata.obs_names, columns=scRNA_adata.obs['cell_type'].unique())\n",
        "\n",
        "# extract the part after the @ sign for matching with spot_coord_df\n",
        "proportions_df.index = [index.split('@')[1] for index in proportions_df.index]\n",
        "\n",
        "# ensure spot_coord_df is aligned with the proportions_df\n",
        "spot_coord_df = spot_coord_df.set_index('barcode')\n",
        "spot_coord_df = spot_coord_df.loc[proportions_df.index]\n",
        "\n",
        "# combine coordinates with proportions\n",
        "plot_df = pd.concat([spot_coord_df[['transformed coords x', 'transformed coords y']], proportions_df], axis=1)\n",
        "\n",
        "# plot the deconvolution results on a scatter plot for each cell type\n",
        "for cell_type in scRNA_adata.obs['cell_type'].unique():\n",
        "    plot_df['proportion'] = plot_df[cell_type]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        plot_df, x='transformed coords x', y='transformed coords y',\n",
        "        color='proportion', color_continuous_scale='viridis',\n",
        "        title=f'Deconvolution of Spatial Transcriptomics Data - {cell_type}',\n",
        "        labels={'proportion': 'Proportion', 'transformed coords x': 'X Coordinate', 'transformed coords y': 'Y Coordinate'})\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6df53a4-428d-414c-889e-554762d561d2",
      "metadata": {
        "scrolled": true,
        "id": "c6df53a4-428d-414c-889e-554762d561d2"
      },
      "outputs": [],
      "source": [
        "# plot deconvolution results on a scatter plot for each cell type\n",
        "for cell_type in scRNA_adata.obs['cell_type'].unique():\n",
        "    plot_df['proportion'] = plot_df[cell_type]\n",
        "\n",
        "    fig = px.scatter(\n",
        "        plot_df, x='transformed coords x', y='transformed coords y',\n",
        "        color='proportion', color_continuous_scale='viridis',\n",
        "        title=f'Deconvolution of Spatial Transcriptomics Data - {cell_type}',\n",
        "        labels={'proportion': 'Proportion', 'transformed coords x': 'X Coordinate', 'transformed coords y': 'Y Coordinate'})\n",
        "\n",
        "    fig.update_layout(width=1200, height=800)\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b9af67-c96f-4eb9-90f3-983aaad50bc9",
      "metadata": {
        "id": "12b9af67-c96f-4eb9-90f3-983aaad50bc9"
      },
      "outputs": [],
      "source": [
        "# combine coordinates with proportions\n",
        "plot_df = pd.concat([spot_coord_df[['transformed coords x', 'transformed coords y']], proportions_df], axis=1)\n",
        "\n",
        "# melt dataframe to long format for Plotly\n",
        "melted_df = plot_df.melt(id_vars=['transformed coords x', 'transformed coords y'], var_name='cell_type', value_name='proportion')\n",
        "\n",
        "# plot deconv results on faceted scatter plot\n",
        "fig = px.scatter(\n",
        "    melted_df, x='transformed coords x', y='transformed coords y',\n",
        "    color='proportion', color_continuous_scale='viridis',\n",
        "    facet_col='cell_type', facet_col_wrap=4,\n",
        "    title='Deconvolution of Spatial Transcriptomics Data',\n",
        "    labels={'proportion': 'Proportion', 'transformed coords x': 'X Coordinate', 'transformed coords y': 'Y Coordinate'})\n",
        "\n",
        "fig.update_layout(width=1400, height=800)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d88abfb-0432-41be-a325-e06df8be4d4b",
      "metadata": {
        "id": "5d88abfb-0432-41be-a325-e06df8be4d4b"
      },
      "source": [
        "## Step X: SPACE-GM Microenvironment Analysis of Inferred scRNAseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4206233-1e9a-41fe-88a4-ba325cf8fc6b",
      "metadata": {
        "id": "f4206233-1e9a-41fe-88a4-ba325cf8fc6b"
      },
      "outputs": [],
      "source": [
        "microe = pd.read_csv(f'scratch/SP_SC_data/microe_annotations_with_names/pre_saved_annotations/pre-treatment-full-20_leiden_0.32-15/{target_acq}.csv', index_col=0)\n",
        "\n",
        "# merge based on CELL_ID\n",
        "merged_microe_df = merged_df.merge(microe, on='CELL_ID')\n",
        "\n",
        "# group by 'MicroE_Name' and 'cell_type' to get counts\n",
        "microe_cell_type_counts = merged_microe_df.groupby(['MicroE_Name', 'cell_type']).size().reset_index(name='counts')\n",
        "\n",
        "microe_cell_type_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de3d72a-e5d1-4ed6-a72e-cc5badfdb28e",
      "metadata": {
        "id": "5de3d72a-e5d1-4ed6-a72e-cc5badfdb28e"
      },
      "outputs": [],
      "source": [
        "# ensure that MicroE_Name is treated as a numeric category for proper ordering\n",
        "microe_cell_type_counts['MicroE_Name'] = pd.Categorical(\n",
        "    microe_cell_type_counts['MicroE_Name'],\n",
        "    categories=[f'MicroE_{i}' for i in range(20)],\n",
        "    ordered=True)\n",
        "\n",
        "# sort the DataFrame by 'MicroE_Name' to ensure correct order\n",
        "microe_cell_type_counts.sort_values('MicroE_Name', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2ee938-9a45-4197-983c-341afc05f894",
      "metadata": {
        "id": "5b2ee938-9a45-4197-983c-341afc05f894"
      },
      "outputs": [],
      "source": [
        "# pivot the data to create a matrix suitable for heatmap\n",
        "pivot_table = microe_cell_type_counts.pivot(index='MicroE_Name', columns='cell_type', values='counts').fillna(0)\n",
        "rows_to_move = pivot_table.iloc[1:3]  # select second and third rows\n",
        "pivot_table_dropped = pivot_table.drop(pivot_table.index[1:3])  # drop the second and third rows from the original\n",
        "pivot_table_reordered = pd.concat([pivot_table_dropped, rows_to_move])  # concatenate the remaining DataFrame with the rows to move\n",
        "pivot_table_reordered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3af8b45e-5b88-47ab-859b-02c6b0f57b9d",
      "metadata": {
        "id": "3af8b45e-5b88-47ab-859b-02c6b0f57b9d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Microenvironment vs Single-Cell Type Counts\")\n",
        "\n",
        "sns.heatmap(pivot_table_reordered, cmap=\"viridis\", annot=True, fmt=\"g\")\n",
        "\n",
        "plt.xlabel(\"Single-Cell Type\")\n",
        "plt.ylabel(\"Microenvironment Name\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a333326b-2c1e-43f3-99a0-245a8bdba5df",
      "metadata": {
        "id": "a333326b-2c1e-43f3-99a0-245a8bdba5df"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=microe_cell_type_counts, x='cell_type', y='counts', hue='MicroE_Name')\n",
        "plt.title(\"Distribution of Single-Cell Types Across Microenvironments\")\n",
        "plt.xlabel(\"Single-Cell Type\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(title='Microenvironment Name')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651a77cf-ffe5-4750-ba3f-45f1873e37b6",
      "metadata": {
        "id": "651a77cf-ffe5-4750-ba3f-45f1873e37b6"
      },
      "outputs": [],
      "source": [
        "# performing GSEA analysis per Leiden cluster of SPACE-GM annot\n",
        "\n",
        "# ensure mod1_indx is aligned with filtered_rna_adata object\n",
        "merged_microe_df['mod1_indx'] = merged_microe_df['mod1_indx'].astype(str)\n",
        "filtered_rna_adata.obs.index = filtered_rna_adata.obs.index.astype(str)\n",
        "\n",
        "# expression data for relevant indices\n",
        "expression_data = filtered_rna_adata[filtered_rna_adata.obs.index.isin(merged_microe_df['mod1_indx'])]\n",
        "\n",
        "# adds MicroE_Cluster as a new column in filtered_rna_adata.obs\n",
        "cluster_mapping = merged_microe_df.set_index('mod1_indx')['MicroE_Cluster'].to_dict()\n",
        "expression_data.obs['MicroE_Cluster'] = expression_data.obs.index.map(cluster_mapping)\n",
        "\n",
        "# adds MicroE_Name for cluster titles\n",
        "cluster_name_mapping = merged_microe_df.set_index('MicroE_Cluster')['MicroE_Name'].to_dict()\n",
        "\n",
        "results = []\n",
        "for cluster in sorted(expression_data.obs['MicroE_Cluster'].unique()):\n",
        "    # filter for current cluster\n",
        "    cluster_data = expression_data[expression_data.obs['MicroE_Cluster'] == cluster]\n",
        "\n",
        "    # prepare rank data using gene names\n",
        "    mean_expression = cluster_data.X.mean(axis=0)\n",
        "    gene_names = cluster_data.var_names\n",
        "\n",
        "    rnk = pd.DataFrame({'gene': gene_names, 'score': mean_expression})\n",
        "    rnk['gene'] = rnk['gene'].str.upper()\n",
        "    rnk = rnk.drop_duplicates(subset=['gene']).sort_values(by='score', ascending=False)\n",
        "\n",
        "    # perform GSEA\n",
        "    if rnk.shape[0] > 0:\n",
        "        gsea_results = gp.prerank(rnk=rnk, gene_sets='MSigDB_Hallmark_2020', min_size=10, max_size=1000, permutation_num=1000)\n",
        "\n",
        "        # Check if results are not empty\n",
        "        if not gsea_results.res2d.empty:\n",
        "            results.append((cluster, gsea_results))\n",
        "    else:\n",
        "        print(f\"No data for cluster: {cluster}\")\n",
        "\n",
        "summary_list = []\n",
        "\n",
        "for cluster, result in results:\n",
        "    summary = result.res2d[['Term', 'ES', 'NES', 'NOM p-val', 'FDR q-val']].copy()\n",
        "    summary['Cluster'] = cluster\n",
        "    summary_list.append(summary)\n",
        "\n",
        "final_summary = pd.concat(summary_list).reset_index(drop=True)\n",
        "\n",
        "# plotting\n",
        "clusters = sorted(final_summary['Cluster'].unique())\n",
        "num_clusters = len(clusters)\n",
        "\n",
        "# set up a 3x3 grid since there are 9 clusters\n",
        "rows, cols = 3, 3\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(24, 18), constrained_layout=True)\n",
        "\n",
        "for i, cluster in enumerate(clusters):\n",
        "    ax = axes[i // cols, i % cols]\n",
        "    cluster_data = final_summary[final_summary['Cluster'] == cluster]\n",
        "    top_terms = cluster_data.sort_values(by='NES', ascending=False).head(10)\n",
        "\n",
        "    sns.barplot(x='NES', y='Term', data=top_terms, ax=ax, palette='viridis')\n",
        "    ax.set_xlabel('Normalized Enrichment Score (NES)')\n",
        "    ax.set_title(cluster_name_mapping.get(cluster, 'Cluster Name Not Found'))\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}